
# game
# boolean = lambda x: bool(['False', 'True'].index(x))
seed: 0 # type=int
area: [64, 64] # nargs=2, type=int, 
view: [12, 12] # type=int, nargs=2
length: 100 # type=int
health: 9 # type=int
window: [1200, 600] # type=int, nargs=2
size: [64, 64] # type=int, nargs=2
record: None # type=str
fps: 5 # type=int
reward: True # type=boolean
wait: False # type=boolean
death: 'reset' # type=str, choices=['continue', 'reset', 'quit'])

task: 'bg' # type=str
fixedAction: False # type=str2bool, default='', help='if you want to have actions in [0,actionMax] set it to True. with False it will set it [actionLow, actionUp]')
observation_data: False # type=str2bool, default=, help='if it is True, then it uses the data that is generated by based on few real world observation')
data_id: 22 # type=int, help='the default item id for the basket dataset')
TLow: 100 # type=int, help='duration of one GAME (lower bound)')
TUp: 100 # type=int, help='duration of one GAME (upper bound)')
demandDistribution: 0 # type=int, help='0=uniform, 1=normal distribution, 2=the sequence of 4,4,4,4,8,..., 3= basket data, 4= forecast data')
scaled: False # type=str2bool, help='if true it uses the (if) existing scaled parameters')
demandLow: [4, 0, 0] # type=int, help='the lower bound of random demand')
demandUp: [16, 8, 6] # type=int, help='the upper bound of random demand')
demandMu: 10 # type=float, help='the mu of the normal distribution for demand ')
demandSigma: 2 # type=float, help='the sigma of the normal distribution for demand ')
actionMax: 2 # type=int, help='it works when fixedAction is True')
actionUp: 2 # type=int, help='bounds on my decision (upper bound), it works when fixedAction is True')
actionLow: -2 # type=int, help='bounds on my decision (lower bound), it works when fixedAction is True')
action_step: 1 # type=int, help='The obtained action value by dnn is multiplied by this value')
actionList: [] # type=list, help='The list of the available actions')
actionListLen: 0 # type=int, help='the length of the action list')
actionListOpt: 0 # type=int, help='the action list which is used in optimal and sterman')
actionListLenOpt:  # type=int, default=0, help='the length of the actionlistopt')
agentTypes: ['dnn','dnn','dnn','dnn'] # type=list, help='the player types')
agent_type1: 'dnn' # type=str, help='the player types for agent 1, it can be dnn, Strm, bs, rnd')
agent_type2: 'dnn' # type=str, help='the player types for agent 2, it can be dnn, Strm, bs, rnd')
agent_type3: 'dnn' # type=str, help='the player types for agent 3, it can be dnn, Strm, bs, rnd')
agent_type4: 'dnn' # type=str, help='the player types for agent 4, it can be dnn, Strm, bs, rnd')
NoAgent: 1 # type=int, help='number of agents, currently it should be in {1,2,3,4}')
cp1: 2.0 # type=float, help='shortage cost of player 1')
cp2: 0.0 # type=float, default=, help='shortage cost of player 2')
cp3: 0.0 # type=float, default=, help='shortage cost of player 3')
cp4: 0.0 # type=float, default=, help='shortage cost of player 4')
c_p: None
ch1: 2.0 # type=float, default=, help='holding cost of player 1')
ch2: 2.0 # type=float, default=, help='holding cost of player 2')
ch3: 2.0 # type=float, default=, help='holding cost of player 3')
ch4: 2.0 # type=float, default=, help='holding cost of player 4')
c_h: None
alpha_b: None
alpha_b1: -0.5 # type=float, help='alpha of Sterman formula parameter for player 1')
alpha_b2: -0.5 # type=float, help='alpha of Sterman formula parameter for player 2')
alpha_b3: -0.5 # type=float, default=, help='alpha of Sterman formula parameter for player 3')
alpha_b4: -0.5 # type=float, default=, help='alpha of Sterman formula parameter for player 4')
betta_b: None
betta_b1: -0.2 # type=float, default=, help='beta of Sterman formula parameter for player 1')
betta_b2: -0.2 # type=float, help='beta of Sterman formula parameter for player 2')
betta_b3: -0.2 # type=float, help='beta of Sterman formula parameter for player 3')
betta_b4: -0.2 # type=float, help='beta of Sterman formula parameter for player 4')
eta: [0,4,4,4] # type=list, help='the total cost regulazer')
distCoeff: 20 # type=int, default=, help='the total cost regulazer')
gameConfig: 3 # type=int, default=, help='if it is "0", it uses the current "agentType", otherwise sets agent types according to the function setAgentType() in this file.')
ifUseTotalReward: False # type=str2bool, default='', help='if you want to have the total rewards in the experience replay, set it to true.')
ifUsedistTotReward: True # type=str2bool, help='If use correction to the rewards in the experience replay for all iterations of current game')
ifUseASAO: True # type=str2bool, help='if use AS and AO, i.e., received shipment and received orders in the input of DNN')
ifUseActionInD: False # type=str2bool, default='', help='if use action in the input of DNN')
stateDim: 5 # type=int, default=, help='Number of elements in the state desciptor - Depends on ifUseASAO')
iftl: False # type=str2bool, help='if apply transfer learning')
ifTransferFromSmallerActionSpace: False # type=str2bool, help='if want to transfer knowledge from a network with different action space size.')
baseActionSize: 5 # type=int, help='if ifTransferFromSmallerActionSpace is true, this determines the size of action space of saved network')
tlBaseBrain: 3 # type=int, help='the gameConfig of the base network for re-training with transfer-learning')
baseDemandDistribution: 0 # type=int, help='same as the demandDistribution')
MultiAgent: False # type=str2bool, help='if run multi-agent RL model, not fully operational')
MultiAgentRun: [True, True, True, True] # type=list, help='In the multi-RL setting, it determines which agent should get training.')
if_use_AS_t_plus_1: False # type=str2bool, help='if use AS[t+1], not AS[t] in the input of DNN')
ifSinglePathExist: False # type=str2bool, help='If true it uses the predefined path in pre_model_dir and does not merge it with demandDistribution.')
ifPlaySavedData: False # type=str2bool, help='If true it uses the saved actions which are read from file.')


### leadtimes ###
leadRecOrderUp_aux: []
leadRecItemUp_aux: []
leadRecItemLow: [2,2,2,4] # type=list, help='the min lead time for receiving items'
leadRecItemUp: [2,2,2,4] # type=list, help='the max lead time for receiving items')
leadRecOrderLow: [2,2,2,0] # type=int, default=, help='the min lead time for receiving orders')
leadRecOrderUp: [2,2,2,0] # type=int, default=, help='the max lead time for receiving orders')
ILInit: [0,0,0,0] # type=list, help='')
AOInit: [0,0,0,0] # type=list, help='')
ASInit: [0,0,0,0] # type=list, help='the initial shipment of each agent')
leadRecItem1: 2 # type=int, help='the min lead time for receiving items')
leadRecItem2: 2 # type=int, help='the min lead time for receiving items')
leadRecItem3: 2 # type=int, help='the min lead time for receiving items')
leadRecItem4: 2 # type=int, , help='the min lead time for receiving items')
leadRecOrder1: 2 # type=int, help='the min lead time for receiving order')
leadRecOrder2: 2 # type=int, help='the min lead time for receiving order')
leadRecOrder3: 2 # type=int, help='the min lead time for receiving order')
leadRecOrder4: 2 # type=int, help='the min lead time for receiving order')
ILInit1: 0 # type=int, help='the initial inventory level of the agent')
ILInit2: 0 # type=int, help='the initial inventory level of the agent')
ILInit3: 0 # type=int, help='the initial inventory level of the agent')
ILInit4: 0 # type=int, help='the initial inventory level of the agent')
AOInit1: 0 # type=int, help='the initial arriving order of the agent')
AOInit2: 0 # type=int, help='the initial arriving order of the agent')
AOInit3: 0 # type=int, help='the initial arriving order of the agent')
AOInit4: 0 # type=int, help='the initial arriving order of the agent')
ASInit1: 0 # type=int, help='the initial arriving shipment of the agent')
ASInit2: 0 # type=int, help='the initial arriving shipment of the agent')
ASInit3: 0 # type=int, help='the initial arriving shipment of the agent')
ASInit4: 0 # type=int, help='the initial arriving shipment of the agent')


### reporting ###
Rsltdnn: [] # type=list, help='the result of dnn play tests will be saved here')
RsltRnd: [] # type=list, help='the result of random play tests will be saved here')
RsltStrm: [] # type=list, help='the result of heuristic fomula play tests will be saved here')
Rsltbs: [] # type=list, help='the result of optimal play tests will be saved here')
ifSaveHist: False # type=str2bool, help='if it is true, saves history, prediction, and the randBatch in each period, WARNING: just make it True in small runs, it saves huge amount of files.')


### DQN ###
maxEpisodesTrain: 60100 # type=int, help='number of GAMES to be trained')
NoHiLayer: 3 # type=int, help='number of hidden layers')
NoFixedLayer: 1 # type=int, help='number of hidden layers')
node1: 180 # type=int, default=, help='the number of nodes in the first hidden layer')
node2: 130 # type=int, help='the number of nodes in the second hidden layer')
node3: 61 # type=int, help='the number of nodes in the third hidden layer')
nodes: [] # type=list, help='')

batchSize: 64 # type=int, default=, help='the batch size which is used to obtain')
minReplayMem: 50000 # type=int, default=, help='the minimum of experience reply size to start dnn')
maxReplayMem: 1000000 # type=int, default=, help='the maximum size of the replay memory')
alpha: .97 # type=float, default=, help='learning rate for total reward distribution ')
gamma: .99 # type=float, default=, help='discount factor for Q-learning')
saveInterval: 10000 # type=int, default=, help='every xx training iteration, saves the games network')
epsilonBeg: 0.9 # type=float, default=, help='')
epsilonEnd: 0.1 # type=float, default=, help='')

lr0: 0.00025 # type=float, help='the learning rate')
Minlr: 1e-8 # type=float, help='the minimum learning rate, if it drops below it, fix it there ')
ifDecayAdam: True # type=str2bool, help='decays the learning rate of the adam optimizer')
decayStep: 10000 # type=int, help='the decay step of the learning rate')
decayRate: 0.98 # type=float, help='the rate to reduce the lr at every decayStep')

display: 1000 # type=int, help='the number of iterations between two display of results.')
momentum: 0.9 # type=float, help='the momentum value')
dnnUpCnt: 10000 # type=int, default=, help='the number of iterations that updates the dnn weights')
multPerdInpt: 10 # type=int, default=, help='Number of history records which we feed into DNN')


### testing ###
testRepeatMid: 50 # type=int, help='it is number of episodes which is going to be used for testing in the middle of training')
testInterval: 100 # type=int, help='every xx games compute "test error"')
ifSaveFigure: True # type=str2bool, help='if is it True, save the figures in each testing.')
if_titled_figure: True # type=str2bool, help='if is it True, save the figures with details in the title.')
saveFigInt: [59990,60000] # type=list, help='')
saveFigIntLow: 59990 # type=int, help='')
saveFigIntUp: 60000 # type=int, default=, help='')
ifsaveHistInterval: False # type=str2bool, default=, help='if every xx games save details of the episode')
saveHistInterval: 50000 # type=int, default=, help='every xx games save details of the play')
Ttest: 100 # type=int, default=, help='it defines the number of periods in the test cases')
ifOptimalSolExist:  # type=str2bool, default=True, help='if the instance has optimal base stock policy, set it to True, otherwise it should be False.')
f1: 8 # type=float,  help='base stock policy decision of player 1')
f2: 8 # type=float, default=8, help='base stock policy decision of player 2')
f3: 0 # type=float, default=0, help='base stock policy decision of player 3')
f4: 0 # type=float, default=0, help='base stock policy decision of player 4')
f: None
f_init: [32,32,32,24] # type=list, help='base stock policy decision for 4 time-steps on the C(4,8) demand distribution')
use_initial_BS: False # type=str2bool, help='If use f_init set it to True')


### utilities ###
address: "" # type=str, help='the address which is used to save the model files')
ifUsePreviousModel: False # type=str2bool, help='if there is a saved model, then False value of this parameter will overwrite.')
number_cpu_active: 5 # type=int, help='number of cpu cores')
gpu_memory_fraction: 0.1 # type=float, help='the fraction of gpu memory which we are gonna use')
# Dirs
load_path: '' # type=str, help='The directory to load the models')
# log_dir:  # type=str, default=os.path.expanduser('./logs/')
# pre_model_dir:  # type=str, default=os.path.expanduser('./pre_model')
# action_dir:  # type=str, default=os.path.expanduser('./'),help='if ifPlaySavedData is true, it uses this path to load actions')
model_dir: './' # type=str, default=
TB: False # type=str2bool, help='set to True if use tensor board and save the required data for TB.')
INFO_print: True # type=str2bool, help='if true, it does not print anything all.')
tbLogInterval: 80000 # type=int, help='number of GAMES for testing')



