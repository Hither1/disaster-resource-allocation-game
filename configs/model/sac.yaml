lr: 0.001 # type=float, metavar='LR', help='learning rate (default: 0.0001)')
gamma: 0.1 # type=float, metavar='G', help='discount factor for rewards (default: 0.99)')
gamma_rate: 0.002 # type=float, metavar='G', help='the increase rate of gamma')
gamma_final: 0.9 # type=float, metavar='G', help='the increase rate of gamma')
tau: 1.00 # type=float, metavar='T', help='parameter for GAE (default: 1.00)')
entropy: 0.005 # type=float, metavar='T', help='parameter for entropy (default: 0.01)')
grad_entropy: 1.0 # type=float, metavar='T', help='parameter for entropy (default: 0.01)')
seed: 1 # type=int, metavar='S', help='random seed (default: 1)')
workers: 6 # type=int, metavar='W', help='how many training processes to use (default: 32)')
A2C_steps: 20 # type=int, metavar='NS', help='number of forward steps in A2C (default: 300)')
env_steps: 20 # type=int, metavar='NS', help='number of steps in one env episode')
start-eps: 2000 # type=int, metavar='NS', help='number of episodes before increasing gamma and env steps')
ToM_train_loops: 1 # type=int, metavar='NS', help='ToM training loops num')
policy_train_loops: 1 # type=int, metavar='NS', help='Policy training loops num')
test_eps: 20 # type=int, metavar='M', help='testing episodes')
ToM_frozen: 5 # type=int, metavar='M', help='episode length of freezing ToM in training')
env: 'IM' # help='environment to train on')
optimizer: 'Adam' # metavar='OPT', help='shares optimizer choice of Adam or RMSprop')
amsgrad: True # metavar='AM', help='Adam optimizer amsgrad parameter')
load_model_dir: null # metavar='LMD', help='folder to load trained models from')
load_executor_dir: null # metavar='LMD', help='folder to load trained low-level policy models from')
log_dir: 'logs/' # metavar='LG', help='folder to save logs')
model: 'ToM2C' # metavar='M', help='ToM2C')
gpu_id: -1 # type=int, nargs='+', help='GPU to use [-1 CPU only]')
norm_reward: true # dest='norm_reward', action='store_true', help='normalize reward')
train_comm: true # dest='train_comm', action='store_true', help='train comm')
random_target: true # dest='random_target', action='store_true', help='random target in MSMTC')
mask_actions: false # dest='mask_actions', action='store_true', help='mask unavailable actions to boost training')
mask: true # dest='mask', action='store_true', help='mask ToM and communication to those out of range')
render: false # dest='render', help='render test')
fix: true # dest='fix', action='store_true', help='fix random seed')
shared_optimizer: true # dest='shared_optimizer', action='store_true', help='use an optimizer without shared statistics.')
train_mode: -1 # type=int, metavar='TM', help='his')
lstm_out: 64 # type=int, metavar='LO', help='lstm output size')
sleep_time: 0 # type=int, metavar='LO', help='seconds')
max_step: 3000000 # type=int, metavar='LO', help='max learning steps')
render_save: true # dest='render_save', action='store_true', help='render save')
num_agents: -1  # type=int; if -1, then the env will load the default setting
num_targets: -1  # type=int; else, you can assign the number of agents and targets yourself