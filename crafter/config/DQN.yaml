####################	DQN setting		####################	
DQN_arg = add_argument_group('DQN')
DQN_arg.add_argument('--maxEpisodesTrain', type=int, default=60100, help='number of GAMES to be trained')
DQN_arg.add_argument('--NoHiLayer', type=int, default=3, help='number of hidden layers')
DQN_arg.add_argument('--NoFixedLayer', type=int, default=1, help='number of hidden layers')
DQN_arg.add_argument('--node1', type=int, default=180, help='the number of nodes in the first hidden layer')
DQN_arg.add_argument('--node2', type=int, default=130, help='the number of nodes in the second hidden layer')
DQN_arg.add_argument('--node3', type=int, default=61, help='the number of nodes in the third hidden layer')
DQN_arg.add_argument('--nodes', type=list, default=[], help='')

DQN_arg.add_argument('--batchSize', type=int, default=64, help='the batch size which is used to obtain')
DQN_arg.add_argument('--minReplayMem', type=int, default=50000, help='the minimum of experience reply size to start dnn')
DQN_arg.add_argument('--maxReplayMem', type=int, default=1000000, help='the maximum size of the replay memory')
DQN_arg.add_argument('--alpha', type=float, default=.97, help='learning rate for total reward distribution ')
DQN_arg.add_argument('--gamma', type=float, default=.99, help='discount factor for Q-learning')
DQN_arg.add_argument('--saveInterval', type=int, default=10000, help='every xx training iteration, saves the games network')
DQN_arg.add_argument('--epsilonBeg', type=float, default=0.9, help='')
DQN_arg.add_argument('--epsilonEnd', type=float, default=0.1, help='')
				
DQN_arg.add_argument('--lr0', type=float, default=0.00025 , help='the learning rate')
DQN_arg.add_argument('--Minlr', type=float, default=1e-8, help='the minimum learning rate, if it drops below it, fix it there ')
DQN_arg.add_argument('--ifDecayAdam', type=str2bool, default=True, help='decays the learning rate of the adam optimizer')
DQN_arg.add_argument('--decayStep', type=int, default=10000, help='the decay step of the learning rate')
DQN_arg.add_argument('--decayRate', type=float, default=0.98, help='the rate to reduce the lr at every decayStep')

DQN_arg.add_argument('--display', type=int, default=1000, help='the number of iterations between two display of results.')
DQN_arg.add_argument('--momentum', type=float, default=0.9, help='the momentum value')
DQN_arg.add_argument('--dnnUpCnt', type=int, default=10000, help='the number of iterations that updates the dnn weights')
DQN_arg.add_argument('--multPerdInpt', type=int, default=10, help='Number of history records which we feed into DNN')