Submodule ToM2C contains modified content
diff --git a/ToM2C/MSMTC/DigitalPose2D/__init__.py b/ToM2C/MSMTC/DigitalPose2D/__init__.py
index a0eab14..5ff12d3 100644
--- a/ToM2C/MSMTC/DigitalPose2D/__init__.py
+++ b/ToM2C/MSMTC/DigitalPose2D/__init__.py
@@ -1,11 +1,11 @@
 # -*- coding: UTF-8 -*-
-from MSMTC.DigitalPose2D.pose_env_base import Pose_Env_Base
+from ToM2C.MSMTC.DigitalPose2D.pose_env_base import Pose_Env_Base
 
 
 class Gym:
     def make(self, env_id, args):
         reset_type = env_id.split('-v')[1]
-        env = Pose_Env_Base(int(reset_type),args)
+        env = Pose_Env_Base(int(reset_type), args)
         return env
 
 
diff --git a/ToM2C/MSMTC/DigitalPose2D/pose_env_base.py b/ToM2C/MSMTC/DigitalPose2D/pose_env_base.py
index 0243b82..186e61b 100644
--- a/ToM2C/MSMTC/DigitalPose2D/pose_env_base.py
+++ b/ToM2C/MSMTC/DigitalPose2D/pose_env_base.py
@@ -8,9 +8,9 @@ from gym import spaces
 import matplotlib.pyplot as plt
 from datetime import datetime
 
-from MSMTC.DigitalPose2D.render import render
-from model import A3C_Single
-from utils import goal_id_filter
+from ToM2C.MSMTC.DigitalPose2D.render import render
+from ToM2C.model import A3C_Single
+from ToM2C.utils import goal_id_filter
 #from main import parser
 import random 
 
@@ -25,11 +25,12 @@ class Pose_Env_Base:
                  ):
         self.nav = nav
         self.reset_type = reset_type
-        self.ENV_PATH = 'MSMTC/DigitalPose2D'
+        self.ENV_PATH = 'ToM2C/MSMTC/DigitalPose2D'
         if setting_path:
             self.SETTING_PATH = setting_path
         else:
             self.SETTING_PATH = os.path.join(self.ENV_PATH, config_path)
+
         with open(self.SETTING_PATH, encoding='utf-8') as f:
             setting = json.load(f)
 
@@ -169,6 +170,7 @@ class Pose_Env_Base:
         y = cam_loc[1] + distance * math.sin(theta)
         return [float(x),float(y)]
         #return [float(np.random.randint(self.start_area[0], self.start_area[1])),float(np.random.randint(self.start_area[2], self.start_area[3]))]
+    
     def obstacle_init_sample(self, loc_a, loc_b):
         d = self.get_distance(loc_a, loc_b)
         R = self.visual_distance
@@ -207,7 +209,6 @@ class Pose_Env_Base:
         return mask_all
     
     def reset(self):
-
         # reset camera
         camera_id_list = [i for i in range(self.n)]
         #random.shuffle(camera_id_list)
@@ -404,7 +405,6 @@ class Pose_Env_Base:
                         #self.random_agents[i].generate_goal(self.random_agents[i].goal_area)
                         delta_x = 0
                         delta_y = 0
-                        #print("collided")
                         break
 
                     if loc[0] + delta_x < self.reset_area[0] or loc[0] + delta_x > self.reset_area[1] or \
@@ -638,8 +638,6 @@ class Pose_Env_Base:
                 angle_h = self.get_hori_direction(cam_loc + cam_rot, self.target_pos_list[j])
                 d = self.get_distance(cam_loc + cam_rot, self.target_pos_list[j])
                 reward, visible = self.angle_reward(angle_h, d)
-                #reward = self.angle_reward(angle_h, d)
-                #visible = self.visible(i,j,d,angle_h)
                 if visible:
                     camera_target_dict[i].append(j)
                     target_camera_dict[j].append(i)
diff --git a/ToM2C/MSMTC/DigitalPose2DBase/__init__.py b/ToM2C/MSMTC/DigitalPose2DBase/__init__.py
index 9ab4f2d..14806c3 100644
--- a/ToM2C/MSMTC/DigitalPose2DBase/__init__.py
+++ b/ToM2C/MSMTC/DigitalPose2DBase/__init__.py
@@ -5,7 +5,7 @@ from MSMTC.DigitalPose2DBase.pose_env_base import Pose_Env_Base
 class Gym:
     def make(self, env_id, render_save):
         reset_type = env_id.split('-v')[1]
-        env = Pose_Env_Base(int(reset_type),render_save=render_save)
+        env = Pose_Env_Base(int(reset_type), render_save=render_save)
         return env
 
 
diff --git a/ToM2C/environment.py b/ToM2C/environment.py
index 92c1a3f..60b9dc9 100644
--- a/ToM2C/environment.py
+++ b/ToM2C/environment.py
@@ -1,10 +1,11 @@
 from __future__ import division
 import numpy as np
 import time
+import crafter
 
 def create_env(env_id, args, rank=-1):
     if 'MSMTC' in env_id:
-        import MSMTC.DigitalPose2D as poseEnv
+        import ToM2C.MSMTC.DigitalPose2D as poseEnv
         env = poseEnv.gym.make(env_id, args)
         # adjust env steps according to args
         env.max_steps = args.env_steps
@@ -20,63 +21,102 @@ def create_env(env_id, args, rank=-1):
         # create multiagent environment
         env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)
         env_wrap = env_wrapper(env, args)
+        return env_wrap
+    elif 'RA' in env_id:  
+        from multiagent.environment import MultiAgentEnv
+        import multiagent.scenarios as scenarios
+        scenario = scenarios.load(args.env + ".py").Scenario()
+        config, unparsed = crafter.config.get_config()
+        world = scenario.make_world(config, args.num_agents, args.num_targets)
+        
+        env = crafter.Env(config, world, scenario.reset_world, scenario.reward, scenario.global_reward, scenario.observation)
+        env = crafter.Recorder(env, config.record)
+        env.reset()
+        env_wrap = env_wrapper(env, args)
+
         return env_wrap
     else:
         raise NotImplementedError
 
 class env_wrapper:
     # wrap for CN low level execution
-    def __init__(self,env,args):
+    def __init__(self, env, args):
         self.env = env
         self.n = self.env.n_agents
         self.num_target = len(self.env.world.landmarks)
-        self.observation_space = np.zeros([self.n, self.num_target, 2])
-        self.action_space = np.zeros([self.n,self.num_target,1])
+        self.observation_space = np.zeros([self.n, self.num_target, self.env.state_dim])
+        self.action_space = np.zeros([self.n, self.num_target, 1])
         self.max_steps = args.env_steps
         self.render = args.render
-        
-    def rule_policy(self,obs):
-        x_rel = obs[0]
-        y_rel = obs[1]
-        if max(abs(x_rel),abs(y_rel)) < 0.05:
-            action = [0]
-        elif abs(x_rel) > abs(y_rel):
-            if x_rel > 0:
-                action = [2]
-            else:
-                action = [1]
-        else:
-            if y_rel > 0:
-                action = [4]
-            else:
-                action = [3]
-        action = np.array(action)
-        return action
 
+    # def step(self, goals_n):
+    #     goals_n = np.squeeze(goals_n)
+    #     keep = 10
+    #     rew_ave = 0
+    #     for step in range(keep):
+    #         # get low level obs
+    #         act_low_n = []
+    #         for i in range(self.n):
+    #             goal = int(goals_n[i])
+    #             land_goal = self.env.world.landmarks[goal]
+    #             agent = self.env.world.agents[i]
+    #             entity_pos = [(land_goal.state.p_pos - agent.state.p_pos)]
+    #             obs_low = np.concatenate(entity_pos)
+    #             act_low_n.append(self.env.world.rule_policy(obs_low))
+            
+    #         obs_n, rew, done_n, info_n = self.env.step(act_low_n)
+    #         if self.render:
+    #             self.env.render()
+    #             time.sleep(0.1)
+    #         rew_ave += rew[0]
+    #     rew_all = np.array([rew_ave/keep])
+    #     return obs_n, rew_all, done_n, info_n
+        
     def step(self, goals_n):
-        #print(goals_n)
         goals_n = np.squeeze(goals_n)
-        keep = 10
-        rew_ave = 0
+        keep = 1
+        rew_ave = []
         for step in range(keep):
-            # get low level obs
-            act_low_n = []
             for i in range(self.n):
                 goal = int(goals_n[i])
                 land_goal = self.env.world.landmarks[goal]
                 agent = self.env.world.agents[i]
-                entity_pos = [(land_goal.state.p_pos - agent.state.p_pos)]
-                obs_low = np.concatenate(entity_pos)
-                act_low_n.append(self.rule_policy(obs_low))
-            
-            obs_n, rew, done_n, info_n = self.env.step(act_low_n)
+                agent._make_decisions_on_requests(self.decode_goal(land_goal))
+
+            obs_n, rew, done_n, info_n = self.env.step()
+
+            if done_n: self.env.reset()
             if self.render:
                 self.env.render()
                 time.sleep(0.1)
-            rew_ave += rew[0]
-        rew_all = np.array([rew_ave/keep])
+            rew_ave.append(rew)
+        rew_all = np.mean(np.array(rew_ave), 0)
+
         return obs_n, rew_all, done_n, info_n
     
+    def decode_goal(self, goal):
+        start = 1
+        goal_ret = {'food': goal[start], 'drink': goal[start+3], 'staff': goal[start+3*2]}
+        return goal_ret
+    
+    # def step(self, goals_n):
+    #     goals_n = np.squeeze(goals_n)
+
+    #     for i in range(self.n):
+    #         goal = int(goals_n[i])
+    #         land_goal = self.env.world.landmarks[goal]
+    #         agent = self.env.world.agents[i]
+    #         agent._make_decisions_on_requests(land_goal)
+            
+    #     obs_n, rew, done_n, info_n = self.env.step()
+    #     if self.render:
+    #             self.env.render()
+    #             time.sleep(0.1)
+
+    #     rew_all = np.array(rew)
+    #     print('obs', len(obs_n), len(obs_n[0]), rew_all.shape, done_n)
+    #     return obs_n, rew_all, done_n, info_n
+    
     def reset(self):
         obs_n = self.env.reset()
         return obs_n
diff --git a/ToM2C/main.py b/ToM2C/main.py
deleted file mode 100644
index 3eca26c..0000000
--- a/ToM2C/main.py
+++ /dev/null
@@ -1,185 +0,0 @@
-from __future__ import print_function, division
-import os
-import time
-import torch
-import argparse
-from datetime import datetime
-import torch.multiprocessing as mp
-
-from test import test
-from train import train
-from worker import worker
-#from train_new import Policy_train
-from model import build_model
-from environment import create_env
-from shared_optim import SharedRMSprop, SharedAdam
-
-os.environ["OMP_NUM_THREADS"] = "1"
-
-parser = argparse.ArgumentParser(description='A3C')
-parser.add_argument('--lr', type=float, default=0.001, metavar='LR', help='learning rate (default: 0.0001)')
-parser.add_argument('--gamma', type=float, default=0.1, metavar='G', help='discount factor for rewards (default: 0.99)')
-parser.add_argument('--gamma-rate', type=float, default=0.002, metavar='G', help='the increase rate of gamma')
-parser.add_argument('--gamma-final', type=float, default=0.9, metavar='G', help='the increase rate of gamma')
-parser.add_argument('--tau', type=float, default=1.00, metavar='T', help='parameter for GAE (default: 1.00)')
-parser.add_argument('--entropy', type=float, default=0.005, metavar='T', help='parameter for entropy (default: 0.01)')
-parser.add_argument('--grad-entropy', type=float, default=1.0, metavar='T', help='parameter for entropy (default: 0.01)')
-parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 1)')
-parser.add_argument('--workers', type=int, default=1, metavar='W', help='how many training processes to use (default: 32)')
-parser.add_argument('--A2C-steps', type=int, default=20, metavar='NS', help='number of forward steps in A2C (default: 300)')
-parser.add_argument('--env-steps', type=int, default=20, metavar='NS', help='number of steps in one env episode')
-parser.add_argument('--start-eps', type=int, default=2000, metavar='NS', help='number of episodes before increasing gamma and env steps')
-parser.add_argument('--ToM-train-loops', type=int, default=1, metavar='NS', help='ToM training loops num')
-parser.add_argument('--policy-train-loops', type=int, default=1, metavar='NS', help='Policy training loops num')
-parser.add_argument('--test-eps', type=int, default=20, metavar='M', help='testing episodes')
-parser.add_argument('--ToM-frozen', type=int, default=5, metavar='M', help='episode length of freezing ToM in training')
-parser.add_argument('--env', default='MSMTC-v3', help='environment to train on')
-parser.add_argument('--optimizer', default='Adam', metavar='OPT', help='shares optimizer choice of Adam or RMSprop')
-parser.add_argument('--amsgrad', default=True, metavar='AM', help='Adam optimizer amsgrad parameter')
-parser.add_argument('--load-model-dir', default=None, metavar='LMD', help='folder to load trained models from')
-parser.add_argument('--load-executor-dir', default=None, metavar='LMD', help='folder to load trained low-level policy models from')
-parser.add_argument('--log-dir', default='logs/', metavar='LG', help='folder to save logs')
-parser.add_argument('--model', default='ToM2C', metavar='M', help='ToM2C')
-parser.add_argument('--gpu-id', type=int, default=-1, nargs='+', help='GPU to use [-1 CPU only] (default: -1)')
-parser.add_argument('--norm-reward', dest='norm_reward', action='store_true', default='True', help='normalize reward')
-parser.add_argument('--train-comm', dest='train_comm', action='store_true', help='train comm')
-parser.add_argument('--random-target', dest='random_target', action='store_true', default='True', help='random target in MSMTC')
-parser.add_argument('--mask-actions', dest='mask_actions', action='store_true', help='mask unavailable actions to boost training')
-parser.add_argument('--mask', dest='mask', action='store_true', help='mask ToM and communication to those out of range')
-parser.add_argument('--render', dest='render', action='store_true', help='render test')
-parser.add_argument('--fix', dest='fix', action='store_true', help='fix random seed')
-parser.add_argument('--shared-optimizer', dest='shared_optimizer', action='store_true', help='use an optimizer without shared statistics.')
-parser.add_argument('--train-mode', type=int, default=-1, metavar='TM', help='his')
-parser.add_argument('--lstm-out', type=int, default=32, metavar='LO', help='lstm output size')
-parser.add_argument('--sleep-time', type=int, default=0, metavar='LO', help='seconds')
-parser.add_argument('--max-step', type=int, default=3000000, metavar='LO', help='max learning steps')
-parser.add_argument('--render_save', dest='render_save', action='store_true', help='render save')
-
-parser.add_argument('--num-agents', type=int, default=-1)   # if -1, then the env will load the default setting
-parser.add_argument('--num-targets', type=int, default=-1)  # else, you can assign the number of agents and targets yourself
-
-# num_step: 20
-# max_step: 500000
-# env_max_step: 100
-# low-level step: 10
-# training mode: -1 for worker collecting trajectories, -10 for workers waiting for training process, -20 for training, -100 for all processes end
-
-def start():
-    args = parser.parse_args()
-    args.shared_optimizer = True
-    if args.gamma_rate == 0:
-        args.gamma = 0.9
-        args.env_steps *= 5
-    if args.gpu_id == -1:
-        torch.manual_seed(args.seed)
-        args.gpu_id = [-1]
-        device_share = torch.device('cpu')
-        mp.set_start_method('spawn')
-    else:
-        torch.cuda.manual_seed(args.seed)
-        mp.set_start_method('spawn', force=True)
-        if len(args.gpu_id) > 1:
-            raise AssertionError("Do not support multi-gpu training")
-            #device_share = torch.device('cpu')
-        else:
-            device_share = torch.device('cuda:' + str(args.gpu_id[-1]))
-    #device_share = torch.device('cuda:0')
-    env = create_env(args.env, args)
-    assert env.max_steps % args.A2C_steps == 0
-    shared_model = build_model(env, args, device_share).to(device_share)
-    shared_model.share_memory()
-    shared_model.train()
-    env.close()
-    del env
-
-    if args.load_model_dir is not None:
-        saved_state = torch.load(
-            args.load_model_dir,
-            map_location=lambda storage, loc: storage)
-        if args.load_model_dir[-3:] == 'pth':
-            shared_model.load_state_dict(saved_state['model'], strict=False)
-        else:
-            shared_model.load_state_dict(saved_state)
-
-    #params = shared_model.parameters()
-    params = []
-    params_ToM = []
-    for name, param in shared_model.named_parameters():
-        if 'ToM' in name or 'other' in name:
-            #print("ToM: ",name)
-            params_ToM.append(param)
-        else:
-            #print("Not ToM: ",name)
-            params.append(param)
-    
-    if args.shared_optimizer:
-        print('share memory')
-        if args.optimizer == 'RMSprop':
-            optimizer_Policy = SharedRMSprop(params, lr=args.lr)
-            if 'ToM' in args.model:
-                optimizer_ToM = SharedRMSprop(params_ToM, lr=args.lr)
-            else:
-                optimizer_ToM = None
-        if args.optimizer == 'Adam':
-            optimizer_Policy = SharedAdam(params, lr=args.lr, amsgrad=args.amsgrad)
-            if 'ToM' in args.model:
-                print("ToM optimizer lr * 10")
-                optimizer_ToM = SharedAdam(params_ToM, lr=args.lr*10, amsgrad=args.amsgrad)
-            else:
-                optimizer_ToM = None
-        optimizer_Policy.share_memory()
-        if optimizer_ToM is not None:
-            optimizer_ToM.share_memory()
-    else:
-        optimizer_Policy = None
-        optimizer_ToM = None
-
-    current_time = datetime.now().strftime('%b%d_%H-%M')
-    args.log_dir = os.path.join(args.log_dir, args.env, current_time)
-
-    processes = []
-    manager = mp.Manager()
-    train_modes = manager.list()
-    n_iters = manager.list()
-    curr_env_steps = manager.list()
-    ToM_count = manager.list()
-    ToM_history = manager.list()
-    Policy_history = manager.list()
-    step_history = manager.list()
-    loss_history = manager.list()
-
-    for rank in range(0, args.workers):
-        p = mp.Process(target=worker, args=(rank, args, shared_model, train_modes, n_iters, curr_env_steps, ToM_count, ToM_history, Policy_history, step_history, loss_history))
-
-        train_modes.append(args.train_mode)
-        n_iters.append(0)
-        curr_env_steps.append(args.env_steps)
-        ToM_count.append(0)
-        ToM_history.append([])
-        Policy_history.append([])
-        step_history.append([])
-        loss_history.append([])
-
-        p.start()
-        processes.append(p)
-        time.sleep(args.sleep_time)
-
-    p = mp.Process(target=test, args=(args, shared_model, optimizer_Policy, optimizer_ToM, train_modes, n_iters))
-    p.start()
-    processes.append(p)
-    time.sleep(args.sleep_time)
-
-    if args.workers > 0:
-        # not only test
-        p = mp.Process(target=train, args=(args, shared_model, optimizer_Policy, optimizer_ToM, train_modes, n_iters, curr_env_steps, ToM_count, ToM_history, Policy_history, step_history, loss_history))
-        p.start()
-        processes.append(p)
-        time.sleep(args.sleep_time)
-
-    for p in processes:
-        time.sleep(args.sleep_time)
-        p.join()
-
-
-if __name__=='__main__':
-    start()
\ No newline at end of file
diff --git a/ToM2C/model.py b/ToM2C/model.py
index 6ac8a45..18c2a52 100644
--- a/ToM2C/model.py
+++ b/ToM2C/model.py
@@ -6,18 +6,18 @@ from gym import spaces
 import torch.nn.functional as F
 from torch.autograd import Variable
 
-from utils import norm_col_init, weights_init
-from perception import NoisyLinear, RNN, AttentionLayer
+from .utils import norm_col_init, weights_init
+from .perception import NoisyLinear, RNN, AttentionLayer
 
 def build_model(env, args, device):
     name = args.model
 
     if "ToM2C" in name and "MSMTC" in args.env:
-        model = ToM2C_multi(env.observation_space,env.action_space,args,device)
+        model = ToM2C_multi(env.observation_space, env.action_space, args, device)
         model.num_agents = env.n
         model.num_targets = env.num_target
     elif "ToM2C" in name and "CN" in args.env:
-        model = ToM2C_single(env.observation_space,env.action_space,args,device)
+        model = ToM2C_single(env.observation_space, env.action_space, args, device)
         model.num_agents = env.n
         model.num_targets = env.num_target
     else:
@@ -191,9 +191,9 @@ class GoalLayer(nn.Module):
         super(GoalLayer,self).__init__()
         self.feature_dim=in_dim
         self.net=nn.Sequential(
-            nn.Linear(in_dim,hidden_dim),
+            nn.Linear(in_dim, hidden_dim),
             nn.ReLU(),
-            nn.Linear(hidden_dim,2),
+            nn.Linear(hidden_dim, 2),
             nn.Softmax(dim=-1)
         )
         self.train()
@@ -206,12 +206,12 @@ class GoalLayer(nn.Module):
 
 class GoalLayer_Single(nn.Module):
     def __init__(self,in_dim, hidden_dim=32, device=torch.device('cpu')):
-        super(GoalLayer_Single,self).__init__()
+        super(GoalLayer_Single, self).__init__()
         self.feature_dim=in_dim
         self.net=nn.Sequential(
-            nn.Linear(in_dim,hidden_dim),
+            nn.Linear(in_dim, hidden_dim),
             nn.ReLU(),
-            nn.Linear(hidden_dim,1),
+            nn.Linear(hidden_dim, 1),
             nn.Softmax(dim=-2)
         )
         self.train()
@@ -224,7 +224,6 @@ class GoalLayer_Single(nn.Module):
 
 class PropNet(nn.Module):
     def __init__(self, node_dim_in, edge_dim_in, hidden_dim, node_dim_out, edge_dim_out, batch_norm=0, pstep=2):
-
         super(PropNet, self).__init__()
 
         self.node_dim_in = node_dim_in
@@ -394,7 +393,7 @@ class Graph_Infer(nn.Module):
         return edge_type_logits, edge_type
 
 class ToM2C_multi(torch.nn.Module):
-    # partial obs + communication + obstacle(MSMTC observation includes obstacles)
+    # partial obs + communication + obstacle (MSMTC observation includes obstacles)
     # each agent can choose multiple targets at the same time
     def __init__(self, obs_space, action_spaces, args, device=torch.device('cpu')):
         super(ToM2C_multi, self).__init__()
@@ -411,7 +410,7 @@ class ToM2C_multi(torch.nn.Module):
         self.attention = AttentionLayer(feature_dim, lstm_out * 2, device)
         feature_dim = self.attention.feature_dim
 
-        self.GRU=RNN(feature_dim ,lstm_out ,1,device,'GRU')
+        self.GRU = RNN(feature_dim ,lstm_out ,1,device,'GRU')
         
         # create ToM, including camera ToM & target ToM
         cam_state_size = 4  # [x, y, cos, sin]
@@ -423,8 +422,8 @@ class ToM2C_multi(torch.nn.Module):
             nn.Linear(lstm_out,1),
             nn.Sigmoid()
         )
-        self.ToM_GRU=RNN(cam_state_size, lstm_out, 1, device, 'GRU')
-        self.other_goal=GoalLayer(lstm_out + self.attention.feature_dim + self.GRU.feature_dim, device=device)
+        self.ToM_GRU = RNN(cam_state_size, lstm_out, 1, device, 'GRU')
+        self.other_goal = GoalLayer(lstm_out + self.attention.feature_dim + self.GRU.feature_dim, device=device)
         
         # GNN based communication inference
         self.graph_infer = Graph_Infer(self.attention.feature_dim + self.ToM_GRU.feature_dim, device=device)
@@ -446,7 +445,7 @@ class ToM2C_multi(torch.nn.Module):
         self.train()
         self.device = device
 
-    def forward(self, multi_obs, self_hiddens, ToM_hiddens, poses, mask, test=False, available_actions = None, train_comm = False):
+    def forward(self, multi_obs, self_hiddens, ToM_hiddens, poses, mask, test=False, available_actions=None, train_comm=False):
         num_agents = self.num_agents
         num_targets = self.num_targets
         if len(multi_obs.size()) != 4:
@@ -498,7 +497,7 @@ class ToM2C_multi(torch.nn.Module):
         att_features, global_feature = self.attention(feature_target) 
         att_features = att_features.reshape(batch_size, num_agents, num_both, -1)
         global_feature = global_feature.reshape(batch_size, num_agents, -1)
-        #features: [batch, num_agents, num_targets, feature_dim]
+        # features: [batch, num_agents, num_targets, feature_dim]
         
         # split features into targets & obstacles
         ############ begin ###########
@@ -509,8 +508,8 @@ class ToM2C_multi(torch.nn.Module):
         h_self = self_hiddens.reshape(1, num_agents * batch_size, -1) # [1, num_agents * batch, hidden_size]
         global_features = global_feature.reshape(num_agents * batch_size, 1, -1) # [num_agents * batch, 1, feature_dim]
         GRU_outputs, hn_self = self.GRU(global_features, h_self)
-        #GRU_outputs=GRU_outputs.squeeze(1) 
-        hn_self=hn_self.reshape(batch_size, num_agents, -1)
+        # GRU_outputs = GRU_outputs.squeeze(1) 
+        hn_self = hn_self.reshape(batch_size, num_agents, -1)
 
         # ToM_input
         # cam_states = poses  #[batch, num_agents, cam_dim]
@@ -547,7 +546,7 @@ class ToM2C_multi(torch.nn.Module):
         obstacle_features = obstacle_features.reshape(batch_size, num_agents, 1, 1, -1).repeat(1, 1, num_agents - 1, num_targets, 1)
         ToM_target_feature = torch.cat((att_feature_expand.detach(), obstacle_features.detach(), ToM_output_expand),-1)
         ToM_target_cover = self.ToM_target(ToM_target_feature) #[b, n, n-1, m, 1]
-        #print(ToM_target_cover)
+
         # ToM_target: Groud Truth Version
         #ToM_target_cover = real_target_cover.unsqueeze(1).expand(batch_size, num_agents, num_agents, num_targets, 1)
         #idx = (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool()
@@ -693,7 +692,6 @@ class ToM2C_single(torch.nn.Module):
     def __init__(self, obs_space, action_spaces, args, device=torch.device('cpu')):
         super(ToM2C_single, self).__init__()
         self.num_agents, num_entity, self.pose_dim = obs_space.shape  # num_targets = target + obstacles
-        #self.num_obstacles = num_entity - self.num_targets
         lstm_out = args.lstm_out
         head_name = args.model
         self.head_name = head_name
@@ -704,7 +702,7 @@ class ToM2C_single(torch.nn.Module):
         self.attention = AttentionLayer(feature_dim, lstm_out, device)
         feature_dim = self.attention.feature_dim
 
-        self.GRU=RNN(feature_dim ,lstm_out ,1,device,'GRU')
+        self.GRU = RNN(feature_dim, lstm_out, 1, device, 'GRU')
 
         # create ToM, including camera ToM & target ToM
         cam_state_size = 4  # [vx, vy, x, y]
@@ -713,21 +711,18 @@ class ToM2C_single(torch.nn.Module):
         self.ToM_target = nn.Sequential(
             nn.Linear(lstm_out + self.attention.feature_dim*2, lstm_out),
             nn.ReLU(),
-            nn.Linear(lstm_out,1),
+            nn.Linear(lstm_out, 1),
             nn.Sigmoid()
         )
-        self.ToM_GRU=RNN(cam_state_size, lstm_out, 1, device, 'GRU')
-        self.other_goal=GoalLayer_Single(lstm_out + self.attention.feature_dim, device=device)#+ self.GRU.feature_dim
+        self.ToM_GRU = RNN(cam_state_size, lstm_out, 1, device, 'GRU')
+        self.other_goal = GoalLayer_Single(lstm_out + self.attention.feature_dim, device=device) #+ self.GRU.feature_dim
 
         # GNN based communication inference
         self.graph_infer = Graph_Infer(self.attention.feature_dim + self.ToM_GRU.feature_dim, device=device)
 
         feature_dim=self.attention.feature_dim #+self.GRU.feature_dim #+ 1 + 2# GRU_output + attention_feature + max_prob + msgs
 
-
         self.reduce_dim = nn.Sequential(
-            #nn.Linear(feature_dim, lstm_out * 2),
-            #nn.ReLU(),
             nn.Linear(feature_dim , lstm_out),
             nn.LeakyReLU(),
         )
@@ -752,24 +747,24 @@ class ToM2C_single(torch.nn.Module):
             multi_obs = multi_obs.unsqueeze(0)
             self_hiddens = self_hiddens.unsqueeze(0)
             ToM_hiddens = ToM_hiddens.unsqueeze(0)
-            #poses = poses.unsqueeze(0)
-            #mask = mask.unsqueeze(0)
         else:
             batch = True
 
         batch_size = multi_obs.size()[0]
         comm_domain = (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool().to(self.device)
-        comm_domain = comm_domain.reshape(1,num_agents,num_agents,1).repeat(batch_size,1,1,1)
-
+        comm_domain = comm_domain.reshape(1, num_agents, num_agents, 1).repeat(batch_size, 1, 1, 1)
         self_vel = Variable(multi_obs[:,:,:2], requires_grad= True)
         self_pos = Variable(multi_obs[:,:,2:4], requires_grad= True)
-        multi_obs = Variable(multi_obs[:,:,4+(num_agents-1)*2:].reshape(batch_size, num_agents, num_targets, 2),requires_grad=True)
+
+        multi_obs = Variable(multi_obs[:,:,4+(num_agents-1)*2:].reshape(batch_size, num_agents, num_targets, 2), requires_grad=True)
+
         obs_dim = multi_obs.size()[-1]
         num_both = multi_obs.size()[2]
 
         # compute
-        target_dis = torch.norm(multi_obs,2,dim=-1) # b*n*m
-        min_dis,_ = torch.min(target_dis, dim=-1)
+        target_dis = torch.norm(multi_obs, 2, dim=-1) # b*n*m
+
+        min_dis, _ = torch.min(target_dis, dim=-1)
         min_dis = min_dis.unsqueeze(-1).repeat(1, 1, num_both)
         # compute real cover: whether target covered by an agent is coverd by another agent.
         # real target coverage, 0.4 = 800(radius)/2000(area size)
@@ -782,8 +777,7 @@ class ToM2C_single(torch.nn.Module):
         real_others_cover = real_target_cover.unsqueeze(1).repeat(1, num_agents, 1, 1, 1)
         real_others_cover = real_others_cover[:,idx].reshape(batch_size, num_agents, num_agents-1, num_targets, 1)
         real_cover = real_others_cover # for ToM target training
-        #multi_obs = Variable(multi_obs, requires_grad=True)
-
+        print('multi_obs', multi_obs.shape)
         feature_target = self.encoder(multi_obs)  # [batch_size, num_agents, num_both, feature_dim]
 
         feature_target = feature_target.reshape(batch_size * num_agents, num_both, -1)
@@ -795,8 +789,8 @@ class ToM2C_single(torch.nn.Module):
 
         # split features into targets & obstacles
         ############ begin ###########
-        #obstacle_features = torch.mean(att_features[:,:,self.num_targets:], 2) if num_both > num_targets else torch.zeros(batch_size, num_agents, att_features.size()[-1]).to(self.device)
-        att_features = att_features[:,:,:self.num_targets]
+        # obstacle_features = torch.mean(att_features[:,:,self.num_targets:], 2) if num_both > num_targets else torch.zeros(batch_size, num_agents, att_features.size()[-1]).to(self.device)
+        att_features = att_features[:, :, :self.num_targets]
         ############# end ############
 
         h_self = self_hiddens.reshape(1, num_agents * batch_size, -1) # [1, num_agents * batch, hidden_size]
@@ -814,14 +808,14 @@ class ToM2C_single(torch.nn.Module):
         #idx = (torch.ones(num_agents,num_agents) - torch.diag(torch.ones(num_agents))).bool()
         #cam_states_duplicate = cam_states_duplicate[:,idx].reshape(batch_size, num_agents, num_agents-1, cam_dim) # [batch,n,n-1,3]
         cam_states_relative = cam_states_duplicate - cam_states.unsqueeze(2).expand(batch_size, num_agents, num_agents, cam_dim)
-        other_pos = cam_states_relative[:,idx].reshape(batch_size, num_agents,(num_agents-1)*cam_dim)
-        other_pos = other_pos.unsqueeze(2).repeat(1,1,num_targets,1) # used for final goal decision
+        other_pos = cam_states_relative[:, idx].reshape(batch_size, num_agents, (num_agents-1)*cam_dim)
+        other_pos = other_pos.unsqueeze(2).repeat(1, 1, num_targets, 1) # used for final goal decision
         camera_states = torch.cat((cam_states_relative, self_vel_duplicate), -1)
 
         # pose mask
         mask = False
         cam_dis = torch.norm(cam_states_relative,p=2,dim=-1)
-        _,sort_id = torch.sort(cam_dis,dim=2)
+        _, sort_id = torch.sort(cam_dis,dim=2)
         sort_id = (sort_id == 1) # can only see nearest 4 poses
         near_dist = (cam_dis[sort_id]).reshape(batch_size,num_agents,1).repeat(1,1,num_agents)
         pose_mask = (cam_dis <= near_dist).unsqueeze(-1)
@@ -829,13 +823,13 @@ class ToM2C_single(torch.nn.Module):
             ToM_goal_mask = pose_mask[:,idx].reshape(batch_size,num_agents,num_agents-1,1,1)
             comm_domain = pose_mask * comm_domain
 
-        #cam_state_theta = ((cam_states_relative[:,:,:,-1]/180) * np.pi).reshape(batch_size, num_agents, num_agents, 1)
-        #camera_states = torch.cat((cam_states_relative[:,:,:,:2], torch.cos(cam_state_theta), torch.sin(cam_state_theta)),-1)
-        camera_states = camera_states.reshape(batch_size*num_agents*(num_agents),1,-1) # [batch*n*n,1,4]
-        h_ToM = ToM_hiddens.reshape(1,-1,self.ToM_GRU.feature_dim) #[1,batch*n*(n),dim]
+        # cam_state_theta = ((cam_states_relative[:,:,:,-1]/180) * np.pi).reshape(batch_size, num_agents, num_agents, 1)
+        # camera_states = torch.cat((cam_states_relative[:,:,:,:2], torch.cos(cam_state_theta), torch.sin(cam_state_theta)),-1)
+        camera_states = camera_states.reshape(batch_size*num_agents*(num_agents), 1, -1) # [batch*n*n,1,4]
+        h_ToM = ToM_hiddens.reshape(1, -1, self.ToM_GRU.feature_dim) #[1,batch*n*(n),dim]
 
         # ToM_camera prediction
-        ToM_output,hn_ToM = self.ToM_GRU(camera_states,h_ToM)
+        ToM_output, hn_ToM = self.ToM_GRU(camera_states, h_ToM)
 
         # GoalLayer input concat
         hn_ToM = hn_ToM.reshape(batch_size, num_agents, num_agents, -1)
@@ -848,56 +842,55 @@ class ToM2C_single(torch.nn.Module):
         GRU_output_expand = GRU_outputs.reshape(batch_size, num_agents, 1, 1, -1)
         GRU_output_expand = GRU_output_expand.expand(batch_size, num_agents, num_agents-1, num_targets, GRU_dim)
         ToM_output_expand = ToM_output_other.expand(batch_size, num_agents, num_agents-1, num_targets, ToM_dim)
-        att_feature_expand = att_features.unsqueeze(2).expand(batch_size,num_agents,num_agents-1,num_targets,att_dim)
-        global_features_expand = global_features.reshape(batch_size, num_agents, 1, 1, -1).repeat(1,1,num_agents-1,num_targets,1)
+        att_feature_expand = att_features.unsqueeze(2).expand(batch_size, num_agents, num_agents-1, num_targets, att_dim)
+        global_features_expand = global_features.reshape(batch_size, num_agents, 1, 1, -1).repeat(1, 1, num_agents-1, num_targets, 1)
         # ToM_target: predicted version
         #camera_states_others = camera_states.reshape(batch_size, num_agents, num_agents, -1)[:,idx]
-        #camera_states = (camera_states_others.reshape(batch_size,num_agents,num_agents-1,1,-1)).repeat(1, 1, 1, num_targets, 1)
-        #obstacle_features = obstacle_features.reshape(batch_size, num_agents, 1, 1, -1).repeat(1, 1, num_agents - 1, num_targets, 1)
+        #camera_states = (camera_states_others.reshape(batch_size, num_agents, num_agents-1, 1, -1)).repeat(1, 1, 1, num_targets, 1)
+
         ToM_target_feature = torch.cat((att_feature_expand.detach(), global_features_expand.detach(), ToM_output_expand),-1)
         ToM_target_cover = self.ToM_target(ToM_target_feature) #[b, n, n-1, m, 1]
 
         # ToM_target: Groud Truth Version
-        #ToM_target_cover = real_target_cover.unsqueeze(1).expand(batch_size, num_agents, num_agents, num_targets, 1)
-        #idx = (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool()
-        #ToM_target_cover = ToM_target_cover[:,idx].reshape(batch_size, num_agents, num_agents-1, num_targets, 1)
-
+        # ToM_target_cover = real_target_cover.unsqueeze(1).expand(batch_size, num_agents, num_agents, num_targets, 1)
+        # idx = (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool()
+        # ToM_target_cover = ToM_target_cover[:,idx].reshape(batch_size, num_agents, num_agents-1, num_targets, 1)
 
         # other goals
-        goal_feature = torch.cat((att_feature_expand.detach(), ToM_output_expand),-1)#GRU_output_expand.detach(), # detach ToM_target here
+        goal_feature = torch.cat((att_feature_expand.detach(), ToM_output_expand), -1)#GRU_output_expand.detach(), # detach ToM_target here
         other_goals = self.other_goal(goal_feature) # [batch, n, n-1, m, 1]
         if mask:
             other_goals = other_goals * ToM_goal_mask
         # prepare masks
         mask = torch.ones([num_agents, num_agents-1]).to(self.device)
-        mask_u = torch.triu(mask,0).reshape(1, num_agents, num_agents-1, 1).repeat(batch_size, 1, 1, num_targets)
-        mask_l = torch.tril(mask,-1).reshape(1, num_agents, num_agents-1, 1).repeat(batch_size, 1, 1, num_targets)
+        mask_u = torch.triu(mask, 0).reshape(1, num_agents, num_agents-1, 1).repeat(batch_size, 1, 1, num_targets)
+        mask_l = torch.tril(mask, -1).reshape(1, num_agents, num_agents-1, 1).repeat(batch_size, 1, 1, num_targets)
         zeros = torch.zeros([batch_size, num_agents, 1, num_targets]).to(self.device)
 
         ############ GNN based communication ################
         other_goals = other_goals.squeeze(-1) #[b,n,n-1,m]
         ToM_goals = other_goals.max(dim=-1)[0]
-        ToM_goals = ToM_goals.reshape(batch_size, num_agents, num_agents-1, 1).repeat(1,1,1,num_targets)
+        ToM_goals = ToM_goals.reshape(batch_size, num_agents, num_agents-1, 1).repeat(1, 1, 1, num_targets)
         ToM_goals = (other_goals >= ToM_goals).detach()
         tri_u_ToM = torch.cat((zeros, ToM_goals * mask_u), 2)
         tri_l_ToM = torch.cat((ToM_goals * mask_l, zeros), 2)
         ToM_goals = tri_u_ToM + tri_l_ToM
         diag_idx = torch.diag(torch.ones(num_agents)).bool()
-        ToM_goals[:,diag_idx] += 1
+        ToM_goals[:, diag_idx] += 1
         ToM_goals = ToM_goals.unsqueeze(-1) #[b, n, n, m, 1]
-        node_features = att_features.unsqueeze(2).repeat(1,1,num_agents,1,1).detach()
+        node_features = att_features.unsqueeze(2).repeat(1, 1, num_agents, 1, 1).detach()
         node_features = torch.sum(node_features * ToM_goals, 3) # sum all targets
-        node_features = torch.cat((node_features, ToM_output.detach()),-1).reshape(batch_size * num_agents, num_agents, -1)
+        node_features = torch.cat((node_features, ToM_output.detach()), -1).reshape(batch_size * num_agents, num_agents, -1)
 
         #edge_logits, comm_edges = self.graph_infer(global_feature.reshape(batch_size,num_agents,-1))
         edge_logits, comm_edges = self.graph_infer(node_features)
-        edge_logits = edge_logits.reshape(batch_size, num_agents, num_agents, num_agents, -1)[:,diag_idx] #[b, n, n, 2]
-        comm_edges = comm_edges.reshape(batch_size, num_agents, num_agents, num_agents, -1)[:,diag_idx]  # [b, n, n, 1]
+        edge_logits = edge_logits.reshape(batch_size, num_agents, num_agents, num_agents, -1)[:, diag_idx] #[b, n, n, 2]
+        comm_edges = comm_edges.reshape(batch_size, num_agents, num_agents, num_agents, -1)[:, diag_idx]  # [b, n, n, 1]
         comm_edges = comm_edges * comm_domain # only communicate with agents in self domain
         comm_domain_reshape = comm_domain.reshape(-1,1).repeat(1,2) #[b*n*n, 2]
 
         # only for ablation test
-        #comm_edges = torch.zeros(batch_size, num_agents, num_agents, 1).to(self.device)
+        # comm_edges = torch.zeros(batch_size, num_agents, num_agents, 1).to(self.device)
 
         edge_logits = edge_logits.reshape(-1,2)[comm_domain_reshape]
         edge_logits = edge_logits.reshape(-1,2) #[k,2] only logits of those edges in comm domain can be saved for training
@@ -922,22 +915,22 @@ class ToM2C_single(torch.nn.Module):
         #end of evaluation
         msgs = comm_msg * comm_target_edges
         msgs = torch.sum(msgs, 1).reshape(batch_size, num_agents, num_targets, 1)
-        msgs = torch.cat((msgs,comm_cnt),-1)
+        msgs = torch.cat((msgs, comm_cnt), -1)
 
         ######### end of GNN based communication ###############
 
         # decide self goals
-        max_prob , _ = torch.max(other_goals,2)
+        max_prob, _ = torch.max(other_goals, 2)
         max_prob = max_prob.reshape(batch_size, num_agents, num_targets, 1).detach() #[batch,n,m,1]
         GRU_outputs = GRU_outputs.reshape(batch_size, num_agents, 1, -1).expand(batch_size, num_agents, num_targets, self.GRU.feature_dim)
         # new version of actor feature, reduce self feature dim
         #self_feature = torch.cat((att_features, GRU_outputs), -1)
         self_feature = self.reduce_dim(att_features)
-        ToM_msgs = torch.cat((max_prob, msgs),-1)
+        ToM_msgs = torch.cat((max_prob, msgs), -1)
         #ToM_msgs = self.raise_dim(ToM_msgs)
         # actor_feature = actor_feature + ToM_msgs
         #feature_target = feature_target.reshape(batch_size,num_agents,num_targets,-1)
-        actor_feature = torch.cat((multi_obs,self_feature,other_pos,ToM_msgs),-1)
+        actor_feature = torch.cat((multi_obs, self_feature, other_pos, ToM_msgs), -1)
         #actor_feature=torch.cat((GRU_outputs, att_features, max_prob, msgs), -1) #[batch,n,m,dim]
         actor_dim = actor_feature.size()[-1]
         critic_feature = torch.sum(actor_feature,2)#.reshape(batch_size, 1, -1).repeat(1, num_agents, 1) #expand(num_agents, num_agents*actor_dim) #[b,n,dim]
@@ -945,7 +938,7 @@ class ToM2C_single(torch.nn.Module):
         # only select target in one's view or received communication
         #print(real_target_cover)
         if available_actions is None:
-            available_actions = (real_target_cover + comm_cnt)>0
+            available_actions = (real_target_cover + comm_cnt) > 0
             available_actions = available_actions.reshape(batch_size * num_agents, num_targets, -1)
         else:
             available_actions = available_actions.reshape(batch_size * num_agents, num_targets, -1)
@@ -954,21 +947,18 @@ class ToM2C_single(torch.nn.Module):
       
         if train_comm:
             zero_msgs = torch.zeros(batch_size, num_agents, num_targets, 3).to(self.device)
-            zero_actor_feature = torch.cat((multi_obs,self_feature,other_pos, zero_msgs),-1).reshape(batch_size * num_agents, num_targets, actor_dim)
+            zero_actor_feature = torch.cat((multi_obs, self_feature, other_pos, zero_msgs),-1).reshape(batch_size * num_agents, num_targets, actor_dim)
             _,_,_,zero_probs = self.actor(zero_actor_feature, test, None)
             probs = probs.reshape(batch_size, num_agents, -1)
-            zero_probs = zero_probs.reshape(batch_size, num_agents,-1)
+            zero_probs = zero_probs.reshape(batch_size, num_agents, -1)
             a = probs.max(-2)[1].unsqueeze(-1)
             b = zero_probs.max(-2)[1].unsqueeze(-1)
-            #print(a.squeeze())
-            #print(b.squeeze())
             
             real_edges = torch.sum(a == b,-1)
             real_edges = 1 - (real_edges == 1).float()
             real_edges = real_edges.unsqueeze(1).repeat(1, num_agents, 1)
             edges_label = real_edges.reshape(-1,1)[comm_domain.reshape(-1,1)]
             #print(edges_label)
-            #print("----------")
             
             return hn_self, hn_ToM, edge_logits, comm_edges.squeeze(-1), real_edges, edges_label
 
@@ -1048,11 +1038,11 @@ class A3C_Single(torch.nn.Module):  # single vision Tracking
         actions, entropies, log_probs, probs = self.actor(feature, test)
         values = self.critic(feature)
 
-        actions = actions.reshape(batch_size, self.n,-1)
-        entropies = entropies.reshape(batch_size,self.n,-1)
-        log_probs = log_probs.reshape(batch_size,self.n,-1)
+        actions = actions.reshape(batch_size, self.n, -1)
+        entropies = entropies.reshape(batch_size,self.n, -1)
+        log_probs = log_probs.reshape(batch_size,self.n, -1)
         probs = probs.reshape(batch_size, self.n, -1)
-        values = values.reshape(batch_size,self.n,-1)
+        values = values.reshape(batch_size, self.n, -1)
         if not batch:
             actions = actions.squeeze(0)
             entropies = entropies.squeeze(0)
diff --git a/ToM2C/multiagent/__init__.py b/ToM2C/multiagent/__init__.py
deleted file mode 100644
index 7427b8d..0000000
--- a/ToM2C/multiagent/__init__.py
+++ /dev/null
@@ -1,18 +0,0 @@
-from gym.envs.registration import register
-
-# Multiagent envs
-# ----------------------------------------
-
-register(
-    id='MultiagentSimple-v0',
-    entry_point='multiagent.envs:SimpleEnv',
-    # FIXME(cathywu) currently has to be exactly max_path_length parameters in
-    # rllab run script
-    max_episode_steps=100,
-)
-
-register(
-    id='MultiagentSimpleSpeakerListener-v0',
-    entry_point='multiagent.envs:SimpleSpeakerListenerEnv',
-    max_episode_steps=100,
-)
diff --git a/ToM2C/multiagent/core.py b/ToM2C/multiagent/core.py
deleted file mode 100644
index 4b8af11..0000000
--- a/ToM2C/multiagent/core.py
+++ /dev/null
@@ -1,221 +0,0 @@
-import numpy as np
-
-# physical/external base state of all entites
-class EntityState(object):
-    def __init__(self):
-        # physical position
-        self.p_pos = None
-        # physical velocity
-        self.p_vel = None
-
-# state of agents (including communication and internal/mental state)
-class AgentState(EntityState):
-    def __init__(self):
-        super(AgentState, self).__init__()
-        # communication utterance
-        self.c = None
-
-# action of the agent
-class Action(object):
-    def __init__(self):
-        # physical action
-        self.u = None
-        # communication action
-        self.c = None
-
-# properties and state of physical world entity
-class Entity(object):
-    def __init__(self):
-        # name 
-        self.name = ''
-        # properties:
-        self.size = 0.050
-        # entity can move / be pushed
-        self.movable = False
-        # entity collides with others
-        self.collide = True
-        # material density (affects mass)
-        self.density = 25.0
-        # color
-        self.color = None
-        # max speed and accel
-        self.max_speed = None
-        self.accel = None
-        # state
-        self.state = EntityState()
-        # mass
-        self.initial_mass = 1.0
-
-    @property
-    def mass(self):
-        return self.initial_mass
-
-# properties of landmark entities
-class Landmark(Entity):
-     def __init__(self):
-        super(Landmark, self).__init__()
-
-# properties of agent entities
-class Agent(Entity):
-    def __init__(self):
-        super(Agent, self).__init__()
-        # agents are movable by default
-        self.movable = True
-        # cannot send communication signals
-        self.silent = False
-        # cannot observe the world
-        self.blind = False
-        # physical motor noise amount
-        self.u_noise = None
-        # communication noise amount
-        self.c_noise = None
-        # control range
-        self.u_range = 1.0
-        # state
-        self.state = AgentState()
-        # action
-        self.action = Action()
-        # script behavior to execute
-        self.action_callback = None
-
-# multi-agent world
-class World(object):
-    def __init__(self):
-        # list of agents and entities (can change at execution-time!)
-        self.agents = []
-        self.landmarks = []
-        self.preys = []
-        # communication channel dimensionality
-        self.dim_c = 0
-        # position dimensionality
-        self.dim_p = 2
-        # position range
-        self.range_p = 1
-        # color dimensionality
-        self.dim_color = 3
-        # simulation timestep
-        self.dt = 0.1
-        # physical damping
-        self.damping = 0.25
-        # contact response parameters
-        self.contact_force = 1e+2
-        self.contact_margin = 1e-3
-        # observation info
-        self.num_agents_obs = 0
-        self.num_landmarks_obs = 0
-        self.num_preys_obs = 0
-
-    # return all entities in the world
-    @property
-    def entities(self):
-        return self.agents + self.preys + self.landmarks 
-
-    # return all agents controllable by external policies
-    @property
-    def policy_agents(self):
-        return [agent for agent in self.agents if agent.action_callback is None]
-
-    # return all preys controllable by external policies
-    @property
-    def policy_preys(self):
-        return [prey for prey in self.preys if prey.action_callback is None]
-
-    # return all agents controlled by world scripts
-    @property
-    def scripted_agents(self):
-        return [agent for agent in self.agents if agent.action_callback is not None]
-
-    # return all preys controlled by world scripts
-    @property
-    def scripted_preys(self):
-        return [prey for prey in self.preys if prey.action_callback is not None]
-
-    # update state of the world
-    def step(self):
-        # set actions for scripted agents and preys
-        for agent in self.scripted_agents:
-            agent.action = agent.action_callback(agent, self)
-        for prey in self.scripted_preys:
-            prey.action = prey.action_callback(prey, self)
-        # gather forces applied to entities
-        p_force = [None] * len(self.entities)
-        # apply agent physical controls
-        p_force = self.apply_action_force(p_force)
-        # apply environment forces
-        p_force = self.apply_environment_force(p_force)
-        # integrate physical state
-        self.integrate_state(p_force)
-        # update agent and prey state
-        for agent in self.agents:
-            self.update_agent_state(agent)
-        for prey in self.preys:
-            self.update_agent_state(prey)
-
-    # gather agent action forces
-    def apply_action_force(self, p_force):
-        # set applied forces
-        for i,agent in enumerate(self.agents):
-            if agent.movable:
-                noise = np.random.randn(*agent.action.u.shape) * agent.u_noise if agent.u_noise else 0.0
-                p_force[i] = agent.action.u + noise  
-        for j,prey in enumerate(self.preys):
-            if prey.movable:
-                noise = np.random.randn(*prey.action.u.shape) * prey.u_noise if prey.u_noise else 0.0
-                p_force[j+len(self.agents)] = prey.action.u + noise                 
-        return p_force
-
-    # gather physical forces acting on entities
-    def apply_environment_force(self, p_force):
-        # simple (but inefficient) collision response
-        for a,entity_a in enumerate(self.entities):
-            for b,entity_b in enumerate(self.entities):
-                if(b <= a): continue
-                [f_a, f_b] = self.get_collision_force(entity_a, entity_b)
-                if(f_a is not None):
-                    if(p_force[a] is None): p_force[a] = 0.0
-                    p_force[a] = f_a + p_force[a] 
-                if(f_b is not None):
-                    if(p_force[b] is None): p_force[b] = 0.0
-                    p_force[b] = f_b + p_force[b]        
-        return p_force
-
-    # integrate physical state
-    def integrate_state(self, p_force):
-        for i,entity in enumerate(self.entities):
-            if not entity.movable: continue
-            entity.state.p_vel = entity.state.p_vel * (1 - self.damping)
-            if (p_force[i] is not None):
-                entity.state.p_vel += (p_force[i] / entity.mass) * self.dt
-            if entity.max_speed is not None:
-                speed = np.sqrt(np.square(entity.state.p_vel[0]) + np.square(entity.state.p_vel[1]))
-                if speed > entity.max_speed:
-                    entity.state.p_vel = entity.state.p_vel / np.sqrt(np.square(entity.state.p_vel[0]) +
-                                                                  np.square(entity.state.p_vel[1])) * entity.max_speed
-            entity.state.p_pos += entity.state.p_vel * self.dt
-
-    def update_agent_state(self, agent):
-        # set communication state (directly for now)
-        if agent.silent:
-            agent.state.c = np.zeros(self.dim_c)
-        else:
-            noise = np.random.randn(*agent.action.c.shape) * agent.c_noise if agent.c_noise else 0.0
-            agent.state.c = agent.action.c + noise      
-
-    # get collision forces for any contact between two entities
-    def get_collision_force(self, entity_a, entity_b):
-        if (not entity_a.collide) or (not entity_b.collide):
-            return [None, None] # not a collider
-        if (entity_a is entity_b):
-            return [None, None] # don't collide against itself
-        # compute actual distance between entities
-        delta_pos = entity_a.state.p_pos - entity_b.state.p_pos
-        dist = np.sqrt(np.sum(np.square(delta_pos)))
-        # minimum allowable distance
-        dist_min = entity_a.size + entity_b.size
-        # softmax penetration
-        k = self.contact_margin
-        penetration = np.logaddexp(0, -(dist - dist_min)/k)*k
-        force = self.contact_force * delta_pos / dist * penetration
-        force_a = +force if entity_a.movable else None
-        force_b = -force if entity_b.movable else None
-        return [force_a, force_b]
\ No newline at end of file
diff --git a/ToM2C/multiagent/environment.py b/ToM2C/multiagent/environment.py
deleted file mode 100644
index 41a58a7..0000000
--- a/ToM2C/multiagent/environment.py
+++ /dev/null
@@ -1,378 +0,0 @@
-import gym
-from gym import spaces
-from gym.envs.registration import EnvSpec
-import numpy as np
-from multiagent.multi_discrete import MultiDiscrete
-
-# environment for all agents in the multiagent world
-# currently code assumes that no agents will be created/destroyed at runtime!
-class MultiAgentEnv(gym.Env):
-    metadata = {
-        'render.modes' : ['human', 'rgb_array']
-    }
-
-    def __init__(self, world, reset_callback=None, reward_callback=None,
-                 observation_callback=None, info_callback=None,
-                 done_callback=None, shared_viewer=True):
-
-        self.world = world
-        self.agents = self.world.policy_agents
-        self.preys = self.world.policy_preys
-        # set required vectorized gym env property
-        self.n_agents = len(world.policy_agents)
-        self.n_landmarks = len(world.landmarks)
-        self.n_landmarks_obs = world.num_landmarks_obs
-        self.n_agents_obs = world.num_agents_obs
-        self.n_preys_obs = world.num_preys_obs
-        # scenario callbacks
-        self.reset_callback = reset_callback
-        self.reward_callback = reward_callback
-        self.observation_callback = observation_callback
-        self.info_callback = info_callback
-        self.done_callback = done_callback
-        # environment parameters
-        self.discrete_action_space = True
-        # if true, action is a number 0...N, otherwise action is a one-hot N-dimensional vector
-        self.discrete_action_input = True
-        # if true, even the action is continuous, action will be performed discretely
-        self.force_discrete_action = world.discrete_action if hasattr(world, 'discrete_action') else False
-        # if true, every agent has the same reward
-        self.shared_reward = world.collaborative if hasattr(world, 'collaborative') else False
-        self.range_p = world.range_p
-        self.dim_p = world.dim_p
-        self.time = 0
-
-        # configure spaces
-        self.action_space = []
-        self.observation_space = []
-        for agent in self.agents:
-            total_action_space = []
-            # physical action space
-            if self.discrete_action_space:
-                u_action_space = spaces.Discrete(world.dim_p * 2 + 1)
-            else:
-                u_action_space = spaces.Box(low=-agent.u_range, high=+agent.u_range, shape=(world.dim_p,), dtype=np.float32)
-            if agent.movable:
-                total_action_space.append(u_action_space)
-            # communication action space
-            if self.discrete_action_space:
-                c_action_space = spaces.Discrete(world.dim_c)
-            else:
-                c_action_space = spaces.Box(low=0.0, high=1.0, shape=(world.dim_c,), dtype=np.float32)
-            if not agent.silent:
-                total_action_space.append(c_action_space)
-            # total action space
-            if len(total_action_space) > 1:
-                # all action spaces are discrete, so simplify to MultiDiscrete action space
-                if all([isinstance(act_space, spaces.Discrete) for act_space in total_action_space]):
-                    act_space = MultiDiscrete([[0, act_space.n - 1] for act_space in total_action_space])
-                else:
-                    act_space = spaces.Tuple(total_action_space)
-                self.action_space.append(act_space)
-            else:
-                self.action_space.append(total_action_space[0])
-            # observation space
-            obs_dim = len(observation_callback(agent, self.world))
-            self.observation_space.append(spaces.Box(low=-np.inf, high=+np.inf, shape=(obs_dim,), dtype=np.float32))
-            agent.action.c = np.zeros(self.world.dim_c)
-
-        # rendering
-        self.shared_viewer = shared_viewer
-        if self.shared_viewer:
-            self.viewers = [None]
-        else:
-            self.viewers = [None] * self.n
-        self._reset_render()
-
-    def bound(self, x):
-        d = np.zeros(2)
-        if abs(x[0])>abs(x[1]) and x[0]<0 and abs(x[0])>0.8*self.range_p:
-            d[0] = 2
-        if abs(x[0])>abs(x[1]) and x[0]>0 and abs(x[0])>0.8*self.range_p:
-            d[0] = -2
-        if abs(x[0])<abs(x[1]) and x[1]<0 and abs(x[1])>0.8*self.range_p:
-            d[1] = 2
-        if abs(x[0])<abs(x[1]) and x[1]>0 and abs(x[1])>0.8*self.range_p:
-            d[1] = -2
-        return d
-
-    def step(self, action_n):
-        obs_n = []
-        reward_n = []
-        done_n = []
-        info_n = {'n': []}
-        self.agents = self.world.policy_agents
-        # set action for each agent
-        for i, agent in enumerate(self.agents):
-            self._set_action(action_n[i], agent, self.action_space[i])
-        # set action for each prey
-        for j, prey in enumerate(self.preys):
-            prey_action = np.zeros(self.action_space[0].n)
-            min_dist = 10000
-            direction = []
-            # move following the oppisite direction of closest agent
-            for agent in self.agents:
-                dist = np.sqrt(np.sum(np.square(prey.state.p_pos - agent.state.p_pos)))
-                if dist < min_dist:
-                    min_dist = dist
-                    direction = (prey.state.p_pos - agent.state.p_pos)/dist
-                    direction_intensity = np.abs(direction)
-                    direction[np.argmax(direction_intensity)] = np.sign(direction[np.argmax(direction_intensity)])*1
-                    direction[np.argmin(direction_intensity)] = 0
-            # not allow to cross the boundary
-            in_bound = self.bound(prey.state.p_pos)
-            prey_action[1] = direction[0] + in_bound[0]
-            prey_action[3] = direction[1] + in_bound[1]
-            # if captured, prey chooses to stay
-            if min_dist <= (prey.size + agent.size):
-                prey_action[0] = 1
-                prey_action[1] = 0
-                prey_action[3] = 0
-            self.force_discrete_action = False
-            self._set_action(prey_action, prey, self.action_space[0])
-            self.force_discrete_action = True
-        # advance world state
-        self.world.step()
-        # record observation for each agent
-        for agent in self.agents:
-            obs_n.append(self._get_obs(agent))
-            r = self._get_reward(agent)
-            reward_n.append(r)
-            done_n.append(self._get_done(agent))
-            info_n['n'].append(self._get_info(agent))
-        # all agents get total reward in cooperative case
-        reward = np.sum(reward_n)
-        if self.shared_reward:
-            reward_n = [reward] * self.n
-        return obs_n, reward_n, done_n, info_n
-
-    def reset(self):
-        # reset world
-        self.reset_callback(self.world)
-        # reset renderer
-        self._reset_render()
-        # record observations for each agent
-        obs_n = []
-        self.agents = self.world.policy_agents
-        for agent in self.agents:
-            obs_n.append(self._get_obs(agent))
-        return obs_n
-
-    # get info used for benchmarking
-    def _get_info(self, agent):
-        if self.info_callback is None:
-            return {}
-        return self.info_callback(agent, self.world)
-
-    # get observation for a particular agent
-    def _get_obs(self, agent):
-        if self.observation_callback is None:
-            return np.zeros(0)
-        return self.observation_callback(agent, self.world)
-
-    # get dones for a particular agent
-    # unused right now -- agents are allowed to go beyond the viewing screen
-    def _get_done(self, agent):
-        if self.done_callback is None:
-            return False
-        return self.done_callback(agent, self.world)
-
-    # get reward for a particular agent
-    def _get_reward(self, agent):
-        if self.reward_callback is None:
-            return 0.0
-        return self.reward_callback(agent, self.world)
-
-    # set env action for a particular agent
-    def _set_action(self, action, agent, action_space, time=None):
-        agent.action.u = np.zeros(self.world.dim_p)
-        agent.action.c = np.zeros(self.world.dim_c)
-        # process action
-        if isinstance(action_space, MultiDiscrete):
-            act = []
-            size = action_space.high - action_space.low + 1
-            index = 0
-            for s in size:
-                act.append(action[index:(index+s)])
-                index += s
-            action = act
-        else:
-            action = [action]
-
-        if agent.movable:
-            # physical action
-            if self.discrete_action_input:
-                agent.action.u = np.zeros(self.world.dim_p)
-                # process discrete action
-                if action[0] == 1: agent.action.u[0] = -1.0
-                if action[0] == 2: agent.action.u[0] = +1.0
-                if action[0] == 3: agent.action.u[1] = -1.0
-                if action[0] == 4: agent.action.u[1] = +1.0
-            else:
-                if self.force_discrete_action:
-                    d = np.argmax(action[0])
-                    action[0][:] = 0.0
-                    action[0][d] = 1.0
-                if self.discrete_action_space:
-                    agent.action.u[0] += action[0][1] - action[0][2]
-                    agent.action.u[1] += action[0][3] - action[0][4]
-                else:
-                    agent.action.u = action[0]
-            sensitivity = 5.0
-            if agent.accel is not None:
-                sensitivity = agent.accel
-            agent.action.u *= sensitivity
-            action = action[1:]
-        if not agent.silent:
-            # communication action
-            if self.discrete_action_input:
-                agent.action.c = np.zeros(self.world.dim_c)
-                agent.action.c[action[0]] = 1.0
-            else:
-                agent.action.c = action[0]
-            action = action[1:]
-        # make sure we used all elements of action
-        assert len(action) == 0
-
-    # reset rendering assets
-    def _reset_render(self):
-        self.render_geoms = None
-        self.render_geoms_xform = None
-
-    # render environment
-    def render(self, mode='human'):
-        if mode == 'human':
-            alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
-            message = ''
-            for agent in self.world.agents:
-                comm = []
-                for other in self.world.agents:
-                    if other is agent: continue
-                    if np.all(other.state.c == 0):
-                        word = '_'
-                    else:
-                        word = alphabet[np.argmax(other.state.c)]
-                    message += (other.name + ' to ' + agent.name + ': ' + word + '   ')
-          
-
-        for i in range(len(self.viewers)):
-            # create viewers (if necessary)
-            if self.viewers[i] is None:
-                # import rendering only if we need it (and don't import for headless machines)
-                #from gym.envs.classic_control import rendering
-                from multiagent import rendering
-                self.viewers[i] = rendering.Viewer(700,700)
-
-        # create rendering geometry
-        if self.render_geoms is None:
-            # import rendering only if we need it (and don't import for headless machines)
-            #from gym.envs.classic_control import rendering
-            from multiagent import rendering
-            self.render_geoms = []
-            self.render_geoms_xform = []
-            for entity in self.world.entities:
-                geom = rendering.make_circle(entity.size)
-                xform = rendering.Transform()
-                if 'agent' in entity.name:
-                    geom.set_color(*entity.color, alpha=0.5)
-                else:
-                    geom.set_color(*entity.color)
-                geom.add_attr(xform)
-                self.render_geoms.append(geom)
-                self.render_geoms_xform.append(xform)
-
-            # add geoms to viewer
-            for viewer in self.viewers:
-                viewer.geoms = []
-                for geom in self.render_geoms:
-                    viewer.add_geom(geom)
-
-        results = []
-        for i in range(len(self.viewers)):
-            from multiagent import rendering
-            # update bounds to center around agent
-            cam_range = 1
-            if self.shared_viewer:
-                pos = np.zeros(self.world.dim_p)
-            else:
-                pos = self.agents[i].state.p_pos
-            self.viewers[i].set_bounds(pos[0]-cam_range,pos[0]+cam_range,pos[1]-cam_range,pos[1]+cam_range)
-            # update geometry positions
-            for e, entity in enumerate(self.world.entities):
-                self.render_geoms_xform[e].set_translation(*entity.state.p_pos)
-            # render to display or array
-            results.append(self.viewers[i].render(return_rgb_array = mode=='rgb_array'))
-
-        return results
-
-    # create receptor field locations in local coordinate frame
-    def _make_receptor_locations(self, agent):
-        receptor_type = 'polar'
-        range_min = 0.05 * 2.0
-        range_max = 1.00
-        dx = []
-        # circular receptive field
-        if receptor_type == 'polar':
-            for angle in np.linspace(-np.pi, +np.pi, 8, endpoint=False):
-                for distance in np.linspace(range_min, range_max, 3):
-                    dx.append(distance * np.array([np.cos(angle), np.sin(angle)]))
-            # add origin
-            dx.append(np.array([0.0, 0.0]))
-        # grid receptive field
-        if receptor_type == 'grid':
-            for x in np.linspace(-range_max, +range_max, 5):
-                for y in np.linspace(-range_max, +range_max, 5):
-                    dx.append(np.array([x,y]))
-        return dx
-
-
-# vectorized wrapper for a batch of multi-agent environments
-# assumes all environments have the same observation and action space
-class BatchMultiAgentEnv(gym.Env):
-    metadata = {
-        'runtime.vectorized': True,
-        'render.modes' : ['human', 'rgb_array']
-    }
-
-    def __init__(self, env_batch):
-        self.env_batch = env_batch
-
-    @property
-    def n(self):
-        return np.sum([env.n for env in self.env_batch])
-
-    @property
-    def action_space(self):
-        return self.env_batch[0].action_space
-
-    @property
-    def observation_space(self):
-        return self.env_batch[0].observation_space
-
-    def step(self, action_n, time):
-        obs_n = []
-        reward_n = []
-        done_n = []
-        info_n = {'n': []}
-        i = 0
-        for env in self.env_batch:
-            obs, reward, done, _ = env.step(action_n[i:(i+env.n)], time)
-            i += env.n
-            obs_n += obs
-            # reward = [r / len(self.env_batch) for r in reward]
-            reward_n += reward
-            done_n += done
-        return obs_n, reward_n, done_n, info_n
-
-    def reset(self):
-        obs_n = []
-        for env in self.env_batch:
-            obs_n += env.reset()
-        return obs_n
-
-    # render environment
-    def render(self, mode='human', close=True):
-        results_n = []
-        for env in self.env_batch:
-            results_n += env.render(mode, close)
-        return results_n
diff --git a/ToM2C/multiagent/multi_discrete.py b/ToM2C/multiagent/multi_discrete.py
deleted file mode 100644
index 05c3e7e..0000000
--- a/ToM2C/multiagent/multi_discrete.py
+++ /dev/null
@@ -1,44 +0,0 @@
-# An old version of OpenAI Gym's multi_discrete.py. (Was getting affected by Gym updates)
-# (https://github.com/openai/gym/blob/1fb81d4e3fb780ccf77fec731287ba07da35eb84/gym/spaces/multi_discrete.py)
-
-import numpy as np
-
-import gym
-from gym.spaces import prng
-
-class MultiDiscrete(gym.Space):
-    """
-    - The multi-discrete action space consists of a series of discrete action spaces with different parameters
-    - It can be adapted to both a Discrete action space or a continuous (Box) action space
-    - It is useful to represent game controllers or keyboards where each key can be represented as a discrete action space
-    - It is parametrized by passing an array of arrays containing [min, max] for each discrete action space
-       where the discrete action space can take any integers from `min` to `max` (both inclusive)
-    Note: A value of 0 always need to represent the NOOP action.
-    e.g. Nintendo Game Controller
-    - Can be conceptualized as 3 discrete action spaces:
-        1) Arrow Keys: Discrete 5  - NOOP[0], UP[1], RIGHT[2], DOWN[3], LEFT[4]  - params: min: 0, max: 4
-        2) Button A:   Discrete 2  - NOOP[0], Pressed[1] - params: min: 0, max: 1
-        3) Button B:   Discrete 2  - NOOP[0], Pressed[1] - params: min: 0, max: 1
-    - Can be initialized as
-        MultiDiscrete([ [0,4], [0,1], [0,1] ])
-    """
-    def __init__(self, array_of_param_array):
-        self.low = np.array([x[0] for x in array_of_param_array])
-        self.high = np.array([x[1] for x in array_of_param_array])
-        self.num_discrete_space = self.low.shape[0]
-
-    def sample(self):
-        """ Returns a array with one sample from each discrete action space """
-        # For each row: round(random .* (max - min) + min, 0)
-        random_array = prng.np_random.rand(self.num_discrete_space)
-        return [int(x) for x in np.floor(np.multiply((self.high - self.low + 1.), random_array) + self.low)]
-    def contains(self, x):
-        return len(x) == self.num_discrete_space and (np.array(x) >= self.low).all() and (np.array(x) <= self.high).all()
-
-    @property
-    def shape(self):
-        return self.num_discrete_space
-    def __repr__(self):
-        return "MultiDiscrete" + str(self.num_discrete_space)
-    def __eq__(self, other):
-        return np.array_equal(self.low, other.low) and np.array_equal(self.high, other.high)
\ No newline at end of file
diff --git a/ToM2C/multiagent/policy.py b/ToM2C/multiagent/policy.py
deleted file mode 100644
index 3d728b0..0000000
--- a/ToM2C/multiagent/policy.py
+++ /dev/null
@@ -1,52 +0,0 @@
-import numpy as np
-from pyglet.window import key
-
-# individual agent policy
-class Policy(object):
-    def __init__(self):
-        pass
-    def action(self, obs):
-        raise NotImplementedError()
-
-# interactive policy based on keyboard input
-# hard-coded to deal only with movement, not communication
-class InteractivePolicy(Policy):
-    def __init__(self, env, agent_index):
-        super(InteractivePolicy, self).__init__()
-        self.env = env
-        # hard-coded keyboard events
-        self.move = [False for i in range(4)]
-        self.comm = [False for i in range(env.world.dim_c)]
-        # register keyboard events with this environment's window
-        env.viewers[agent_index].window.on_key_press = self.key_press
-        env.viewers[agent_index].window.on_key_release = self.key_release
-
-    def action(self, obs):
-        # ignore observation and just act based on keyboard events
-        if self.env.discrete_action_input:
-            u = 0
-            if self.move[0]: u = 1
-            if self.move[1]: u = 2
-            if self.move[2]: u = 4
-            if self.move[3]: u = 3
-        else:
-            u = np.zeros(5) # 5-d because of no-move action
-            if self.move[0]: u[1] += 1.0
-            if self.move[1]: u[2] += 1.0
-            if self.move[3]: u[3] += 1.0
-            if self.move[2]: u[4] += 1.0
-            if True not in self.move:
-                u[0] += 1.0
-        return np.concatenate([u, np.zeros(self.env.world.dim_c)])
-
-    # keyboard event callbacks
-    def key_press(self, k, mod):
-        if k==key.LEFT:  self.move[0] = True
-        if k==key.RIGHT: self.move[1] = True
-        if k==key.UP:    self.move[2] = True
-        if k==key.DOWN:  self.move[3] = True
-    def key_release(self, k, mod):
-        if k==key.LEFT:  self.move[0] = False
-        if k==key.RIGHT: self.move[1] = False
-        if k==key.UP:    self.move[2] = False
-        if k==key.DOWN:  self.move[3] = False
diff --git a/ToM2C/multiagent/rendering.py b/ToM2C/multiagent/rendering.py
deleted file mode 100644
index c4076b5..0000000
--- a/ToM2C/multiagent/rendering.py
+++ /dev/null
@@ -1,345 +0,0 @@
-"""
-2D rendering framework
-"""
-from __future__ import division
-import os
-import six
-import sys
-
-if "Apple" in sys.version:
-    if 'DYLD_FALLBACK_LIBRARY_PATH' in os.environ:
-        os.environ['DYLD_FALLBACK_LIBRARY_PATH'] += ':/usr/lib'
-        # (JDS 2016/04/15): avoid bug on Anaconda 2.3.0 / Yosemite
-
-from gym.utils import reraise
-from gym import error
-
-try:
-    import pyglet
-except ImportError as e:
-    reraise(suffix="HINT: you can install pyglet directly via 'pip install pyglet'. But if you really just want to install all Gym dependencies and not have to think about it, 'pip install -e .[all]' or 'pip install gym[all]' will do it.")
-
-try:
-    from pyglet.gl import *
-except ImportError as e:
-    reraise(prefix="Error occured while running `from pyglet.gl import *`",suffix="HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>'")
-
-import math
-import numpy as np
-
-RAD2DEG = 57.29577951308232
-
-def get_display(spec):
-    """Convert a display specification (such as :0) into an actual Display
-    object.
-
-    Pyglet only supports multiple Displays on Linux.
-    """
-    if spec is None:
-        return None
-    elif isinstance(spec, six.string_types):
-        return pyglet.canvas.Display(spec)
-    else:
-        raise error.Error('Invalid display specification: {}. (Must be a string like :0 or None.)'.format(spec))
-
-class Viewer(object):
-    def __init__(self, width, height, display=None):
-        display = get_display(display)
-
-        self.width = width
-        self.height = height
-
-        self.window = pyglet.window.Window(width=width, height=height, display=display)
-        self.window.on_close = self.window_closed_by_user
-        self.geoms = []
-        self.onetime_geoms = []
-        self.transform = Transform()
-
-        glEnable(GL_BLEND)
-        # glEnable(GL_MULTISAMPLE)
-        glEnable(GL_LINE_SMOOTH)
-        # glHint(GL_LINE_SMOOTH_HINT, GL_DONT_CARE)
-        glHint(GL_LINE_SMOOTH_HINT, GL_NICEST)
-        glLineWidth(2.0)
-        glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)
-
-    def close(self):
-        self.window.close()
-
-    def window_closed_by_user(self):
-        self.close()
-
-    def set_bounds(self, left, right, bottom, top):
-        assert right > left and top > bottom
-        scalex = self.width/(right-left)
-        scaley = self.height/(top-bottom)
-        self.transform = Transform(
-            translation=(-left*scalex, -bottom*scaley),
-            scale=(scalex, scaley))
-
-    def add_geom(self, geom):
-        self.geoms.append(geom)
-
-    def add_onetime(self, geom):
-        self.onetime_geoms.append(geom)
-
-    def render(self, return_rgb_array=False):
-        glClearColor(1,1,1,1)
-        self.window.clear()
-        self.window.switch_to()
-        self.window.dispatch_events()
-        self.transform.enable()
-        for geom in self.geoms:
-            geom.render()
-        for geom in self.onetime_geoms:
-            geom.render()
-        self.transform.disable()
-        arr = None
-        if return_rgb_array:
-            buffer = pyglet.image.get_buffer_manager().get_color_buffer()
-            image_data = buffer.get_image_data()
-            arr = np.fromstring(image_data.data, dtype=np.uint8, sep='')
-            # In https://github.com/openai/gym-http-api/issues/2, we
-            # discovered that someone using Xmonad on Arch was having
-            # a window of size 598 x 398, though a 600 x 400 window
-            # was requested. (Guess Xmonad was preserving a pixel for
-            # the boundary.) So we use the buffer height/width rather
-            # than the requested one.
-            arr = arr.reshape(buffer.height, buffer.width, 4)
-            arr = arr[::-1,:,0:3]
-        self.window.flip()
-        self.onetime_geoms = []
-        return arr
-
-    # Convenience
-    def draw_circle(self, radius=10, res=30, filled=True, **attrs):
-        geom = make_circle(radius=radius, res=res, filled=filled)
-        _add_attrs(geom, attrs)
-        self.add_onetime(geom)
-        return geom
-
-    def draw_polygon(self, v, filled=True, **attrs):
-        geom = make_polygon(v=v, filled=filled)
-        _add_attrs(geom, attrs)
-        self.add_onetime(geom)
-        return geom
-
-    def draw_polyline(self, v, **attrs):
-        geom = make_polyline(v=v)
-        _add_attrs(geom, attrs)
-        self.add_onetime(geom)
-        return geom
-
-    def draw_line(self, start, end, **attrs):
-        geom = Line(start, end)
-        _add_attrs(geom, attrs)
-        self.add_onetime(geom)
-        return geom
-
-    def get_array(self):
-        self.window.flip()
-        image_data = pyglet.image.get_buffer_manager().get_color_buffer().get_image_data()
-        self.window.flip()
-        arr = np.fromstring(image_data.data, dtype=np.uint8, sep='')
-        arr = arr.reshape(self.height, self.width, 4)
-        return arr[::-1,:,0:3]
-
-def _add_attrs(geom, attrs):
-    if "color" in attrs:
-        geom.set_color(*attrs["color"])
-    if "linewidth" in attrs:
-        geom.set_linewidth(attrs["linewidth"])
-
-class Geom(object):
-    def __init__(self):
-        self._color=Color((0, 0, 0, 1.0))
-        self.attrs = [self._color]
-    def render(self):
-        for attr in reversed(self.attrs):
-            attr.enable()
-        self.render1()
-        for attr in self.attrs:
-            attr.disable()
-    def render1(self):
-        raise NotImplementedError
-    def add_attr(self, attr):
-        self.attrs.append(attr)
-    def set_color(self, r, g, b, alpha=1):
-        self._color.vec4 = (r, g, b, alpha)
-
-class Attr(object):
-    def enable(self):
-        raise NotImplementedError
-    def disable(self):
-        pass
-
-class Transform(Attr):
-    def __init__(self, translation=(0.0, 0.0), rotation=0.0, scale=(1,1)):
-        self.set_translation(*translation)
-        self.set_rotation(rotation)
-        self.set_scale(*scale)
-    def enable(self):
-        glPushMatrix()
-        glTranslatef(self.translation[0], self.translation[1], 0) # translate to GL loc ppint
-        glRotatef(RAD2DEG * self.rotation, 0, 0, 1.0)
-        glScalef(self.scale[0], self.scale[1], 1)
-    def disable(self):
-        glPopMatrix()
-    def set_translation(self, newx, newy):
-        self.translation = (float(newx), float(newy))
-    def set_rotation(self, new):
-        self.rotation = float(new)
-    def set_scale(self, newx, newy):
-        self.scale = (float(newx), float(newy))
-
-class Color(Attr):
-    def __init__(self, vec4):
-        self.vec4 = vec4
-    def enable(self):
-        glColor4f(*self.vec4)
-
-class LineStyle(Attr):
-    def __init__(self, style):
-        self.style = style
-    def enable(self):
-        glEnable(GL_LINE_STIPPLE)
-        glLineStipple(1, self.style)
-    def disable(self):
-        glDisable(GL_LINE_STIPPLE)
-
-class LineWidth(Attr):
-    def __init__(self, stroke):
-        self.stroke = stroke
-    def enable(self):
-        glLineWidth(self.stroke)
-
-class Point(Geom):
-    def __init__(self):
-        Geom.__init__(self)
-    def render1(self):
-        glBegin(GL_POINTS) # draw point
-        glVertex3f(0.0, 0.0, 0.0)
-        glEnd()
-
-class FilledPolygon(Geom):
-    def __init__(self, v):
-        Geom.__init__(self)
-        self.v = v
-    def render1(self):
-        if   len(self.v) == 4 : glBegin(GL_QUADS)
-        elif len(self.v)  > 4 : glBegin(GL_POLYGON)
-        else: glBegin(GL_TRIANGLES)
-        for p in self.v:
-            glVertex3f(p[0], p[1],0)  # draw each vertex
-        glEnd()
-
-        color = (self._color.vec4[0] * 0.5, self._color.vec4[1] * 0.5, self._color.vec4[2] * 0.5, self._color.vec4[3] * 0.5)
-        glColor4f(*color)
-        glBegin(GL_LINE_LOOP)
-        for p in self.v:
-            glVertex3f(p[0], p[1],0)  # draw each vertex
-        glEnd()
-
-def make_circle(radius=10, res=30, filled=True):
-    points = []
-    for i in range(res):
-        ang = 2*math.pi*i / res
-        points.append((math.cos(ang)*radius, math.sin(ang)*radius))
-    if filled:
-        return FilledPolygon(points)
-    else:
-        return PolyLine(points, True)
-
-def make_polygon(v, filled=True):
-    if filled: return FilledPolygon(v)
-    else: return PolyLine(v, True)
-
-def make_polyline(v):
-    return PolyLine(v, False)
-
-def make_capsule(length, width):
-    l, r, t, b = 0, length, width/2, -width/2
-    box = make_polygon([(l,b), (l,t), (r,t), (r,b)])
-    circ0 = make_circle(width/2)
-    circ1 = make_circle(width/2)
-    circ1.add_attr(Transform(translation=(length, 0)))
-    geom = Compound([box, circ0, circ1])
-    return geom
-
-class Compound(Geom):
-    def __init__(self, gs):
-        Geom.__init__(self)
-        self.gs = gs
-        for g in self.gs:
-            g.attrs = [a for a in g.attrs if not isinstance(a, Color)]
-    def render1(self):
-        for g in self.gs:
-            g.render()
-
-class PolyLine(Geom):
-    def __init__(self, v, close):
-        Geom.__init__(self)
-        self.v = v
-        self.close = close
-        self.linewidth = LineWidth(1)
-        self.add_attr(self.linewidth)
-    def render1(self):
-        glBegin(GL_LINE_LOOP if self.close else GL_LINE_STRIP)
-        for p in self.v:
-            glVertex3f(p[0], p[1],0)  # draw each vertex
-        glEnd()
-    def set_linewidth(self, x):
-        self.linewidth.stroke = x
-
-class Line(Geom):
-    def __init__(self, start=(0.0, 0.0), end=(0.0, 0.0)):
-        Geom.__init__(self)
-        self.start = start
-        self.end = end
-        self.linewidth = LineWidth(1)
-        self.add_attr(self.linewidth)
-
-    def render1(self):
-        glBegin(GL_LINES)
-        glVertex2f(*self.start)
-        glVertex2f(*self.end)
-        glEnd()
-
-class Image(Geom):
-    def __init__(self, fname, width, height):
-        Geom.__init__(self)
-        self.width = width
-        self.height = height
-        img = pyglet.image.load(fname)
-        self.img = img
-        self.flip = False
-    def render1(self):
-        self.img.blit(-self.width/2, -self.height/2, width=self.width, height=self.height)
-
-# ================================================================
-
-class SimpleImageViewer(object):
-    def __init__(self, display=None):
-        self.window = None
-        self.isopen = False
-        self.display = display
-    def imshow(self, arr):
-        if self.window is None:
-            height, width, channels = arr.shape
-            self.window = pyglet.window.Window(width=width, height=height, display=self.display)
-            self.width = width
-            self.height = height
-            self.isopen = True
-        assert arr.shape == (self.height, self.width, 3), "You passed in an image with the wrong number shape"
-        image = pyglet.image.ImageData(self.width, self.height, 'RGB', arr.tobytes(), pitch=self.width * -3)
-        self.window.clear()
-        self.window.switch_to()
-        self.window.dispatch_events()
-        image.blit(0,0)
-        self.window.flip()
-    def close(self):
-        if self.isopen:
-            self.window.close()
-            self.isopen = False
-    def __del__(self):
-        self.close()
\ No newline at end of file
diff --git a/ToM2C/multiagent/scenario.py b/ToM2C/multiagent/scenario.py
deleted file mode 100644
index 6fc1b20..0000000
--- a/ToM2C/multiagent/scenario.py
+++ /dev/null
@@ -1,10 +0,0 @@
-import numpy as np
-
-# defines scenario upon which the world is built
-class BaseScenario(object):
-    # create elements of the world
-    def make_world(self):
-        raise NotImplementedError()
-    # create initial conditions of the world
-    def reset_world(self, world):
-        raise NotImplementedError()
diff --git a/ToM2C/multiagent/scenarios/CN.py b/ToM2C/multiagent/scenarios/CN.py
deleted file mode 100644
index 20bd307..0000000
--- a/ToM2C/multiagent/scenarios/CN.py
+++ /dev/null
@@ -1,137 +0,0 @@
-import numpy as np
-from multiagent.core import World, Agent, Landmark
-from multiagent.scenario import BaseScenario
-import random
-
-
-class Scenario(BaseScenario):
-    def make_world(self, num_agents, num_targets):
-        world = World()
-        # set any world properties first
-        world.dim_c = 0
-        if num_agents == -1:
-            num_agents = 3
-            num_landmarks = 3
-        else:
-            if num_targets == -1:
-                raise AssertionError("Number of targets is not assigned")
-            else:
-                num_landmarks = num_targets
-        world.collaborative = False
-        world.discrete_action = True
-        world.num_agents_obs = 2
-        world.num_landmarks_obs = 2
-        # add agents
-        world.agents = [Agent() for i in range(num_agents)]
-        for i, agent in enumerate(world.agents):
-            agent.name = 'agent %d' % i
-            agent.collide = True
-            agent.silent = True
-            agent.size = 0.05
-        # add landmarks
-        world.landmarks = [Landmark() for i in range(num_landmarks)]
-        for i, landmark in enumerate(world.landmarks):
-            landmark.name = 'landmark %d' % i
-            landmark.collide = False
-            landmark.movable = False
-        # make initial conditions
-        self.reset_world(world)
-        return world
-
-    def reset_world(self, world):
-        # random properties for agents
-        for i, agent in enumerate(world.agents):
-            agent.color = np.array([0.35, 0.35, 0.85])
-        # random properties for landmarks
-        for i, landmark in enumerate(world.landmarks):
-            landmark.color = np.array([0.25, 0.25, 0.25])
-        # set random initial states
-        for agent in world.agents:
-            agent.state.p_pos = np.random.uniform(-world.range_p, +world.range_p, world.dim_p)
-            agent.state.p_vel = np.zeros(world.dim_p)
-            agent.state.c = np.zeros(world.dim_c)
-        for i, landmark in enumerate(world.landmarks):
-            landmark.state.p_pos = np.random.uniform(-world.range_p, +world.range_p, world.dim_p)
-            if i != 0:
-                for j in range(i): 
-                    while True:
-                        if np.sqrt(np.sum(np.square(landmark.state.p_pos - world.landmarks[j].state.p_pos)))>0.22:
-                            break
-                        else: landmark.state.p_pos = np.random.uniform(-world.range_p, +world.range_p, world.dim_p)
-            landmark.state.p_vel = np.zeros(world.dim_p)
-        
-        # # set agent goals
-        # if goals is None:
-        #     goals = [i for i in range(len(world.agents))]
-        #     random.shuffle(goals)
-        # world.goals = goals
-
-    def benchmark_data(self, agent, world):
-        rew = 0
-        collisions = 0
-        occupied_landmarks = 0
-        min_dists = 0
-        for l in world.landmarks:
-            collision_dist = agent.size + l.size
-            dists = [np.sqrt(np.sum(np.square(a.state.p_pos - l.state.p_pos))) for a in world.agents]
-            min_dists += min(dists)
-            rew -= min(dists)
-            if min(dists) < collision_dist:
-                occupied_landmarks += 1
-        if agent.collide:
-            for a in world.agents:
-                for b in world.agents:
-                    if a is b: continue
-                    if self.is_collision(a, b):
-                        collisions += 0.5
-        return (rew, collisions, min_dists, occupied_landmarks)
-
-    def is_collision(self, agent1, agent2):
-        delta_pos = agent1.state.p_pos - agent2.state.p_pos
-        dist = np.sqrt(np.sum(np.square(delta_pos)))
-        collision_dist = agent1.size + agent2.size
-        return True if dist < collision_dist else False
-
-    def reward(self, agent, world):
-        # Agents are rewarded based on minimum agent distance to each landmark, penalized for collisions
-        rew = 0
-        # local reward
-        #dists = [np.sqrt(np.sum(np.square(agent.state.p_pos - l.state.p_pos))) for l in world.landmarks]
-        #rew = rew - min(dists)
-        # global reward
-        for l in world.landmarks:
-            dists = [np.sqrt(np.sum(np.square(a.state.p_pos - l.state.p_pos))) for a in world.agents]
-            rew -= min(dists)
-        # collisions penalty
-        if agent.collide:
-            for a in world.agents:
-                for b in world.agents:
-                    if a is b: continue
-                    if self.is_collision(a, b):
-                        rew -= 0.5
-        return rew
-
-    def observation(self, agent, world):
-        entity_pos = []
-        dist_n = []
-        for entity in world.landmarks:  # world.entities:
-            entity_pos.append(entity.state.p_pos - agent.state.p_pos)
-            dist_n.append(np.sqrt(np.sum(np.square(agent.state.p_pos - entity.state.p_pos))))
-        # dist_sort = dist_n.copy()
-        # dist_sort.sort()
-        # num_landmarks_obs = world.num_landmarks_obs
-        # dist_thresh = dist_sort[num_landmarks_obs-1]
-        target_pos = []
-        for i,pos in enumerate(entity_pos):
-            if True:#dist_n[i] <= dist_thresh:
-                target_pos.append(pos)
-            else:
-                target_pos.append(np.array([100,100]))
-        other_pos = []
-        for other in world.agents:
-            if other is agent: continue
-            other_pos.append(other.state.p_pos - agent.state.p_pos)
-        #print(target_pos)
-        return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + other_pos + target_pos)
-
-
diff --git a/ToM2C/multiagent/scenarios/__init__.py b/ToM2C/multiagent/scenarios/__init__.py
deleted file mode 100644
index bf562e7..0000000
--- a/ToM2C/multiagent/scenarios/__init__.py
+++ /dev/null
@@ -1,7 +0,0 @@
-import imp
-import os.path as osp
-
-
-def load(name):
-    pathname = osp.join(osp.dirname(__file__), name)
-    return imp.load_source('', pathname)
diff --git a/ToM2C/player_util.py b/ToM2C/player_util.py
index ca49501..4faaa8e 100644
--- a/ToM2C/player_util.py
+++ b/ToM2C/player_util.py
@@ -6,7 +6,7 @@ import torch.nn.functional as F
 import numpy as np
 from torch.autograd import Variable
 import json
-from utils import ensure_shared_grads
+from .utils import ensure_shared_grads
 
 class Agent(object):
     def __init__(self, model, env, args, state, device):
@@ -94,12 +94,15 @@ class Agent(object):
 
         self.poses = self.get_other_poses()
         self.mask = self.get_mask()
+        # value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goals, edge_logits, comm_edges, probs, real_cover, ToM_target_cover =\
+        #     self.model(self.state, self.hself, self.hToM, self.poses, self.mask, available_actions = available_actions)
         value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goals, edge_logits, comm_edges, probs, real_cover, ToM_target_cover =\
-            self.model(self.state, self.hself, self.hToM, self.poses, self.mask, available_actions = available_actions)
+            self.model(self.state, self.hself, self.hToM, available_actions=available_actions)
         
         actions_env = actions.cpu().numpy() # only ndarrays can be processed by the environment
         state_multi, reward, self.done, self.info = self.env.step(actions_env)#,obstacle=True)
-        reward_multi = reward.repeat(self.num_agents) # all agents share the same reward
+        # reward_multi = reward.repeat(self.num_agents) # all agents share the same reward
+        reward_multi = reward
 
         self.reward_org = reward_multi.copy()
 
@@ -107,11 +110,18 @@ class Agent(object):
             reward_multi = self.reward_normalizer(reward_multi)
 
         # save state for training
-        Policy_data = {"state":self.state.detach().cpu().numpy(), "poses": self.poses.detach().cpu().numpy(),"actions": actions_env, "reward": reward_multi,\
-            "mask":self.mask.detach().cpu().numpy(),"available_actions": available_actions_data}
+        Policy_data = {"state": self.state.detach().cpu().numpy(), 
+                    #    "poses": self.poses.detach().cpu().numpy(),
+                       "actions": actions_env, 
+                       "reward": reward_multi,
+                        "mask":self.mask.detach().cpu().numpy(),
+                        "available_actions": available_actions_data}
         real_goals = torch.cat((1-actions,actions),-1)
-        ToM_data = {"state":self.state.detach().cpu().numpy(), "poses":self.poses.detach().cpu().numpy(), "mask":self.mask.detach().cpu().numpy(),\
-            "real":real_goals.detach().cpu().numpy(), "available_actions": available_actions_data}
+        ToM_data = {"state":self.state.detach().cpu().numpy(), 
+                    # "poses":self.poses.detach().cpu().numpy(), 
+                    "mask":self.mask.detach().cpu().numpy(),\
+                    "real":real_goals.detach().cpu().numpy(), 
+                    "available_actions": available_actions_data}
         self.Policy_history.append(Policy_data)
         self.ToM_history.append(ToM_data)
 
@@ -137,8 +147,12 @@ class Agent(object):
         with torch.no_grad():
             self.poses = self.get_other_poses()
             self.mask = self.get_mask()
-            value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goals, edge_logits, comm_edges, probs, real_cover, ToM_target_cover=\
-                self.model(self.state, self.hself, self.hToM, self.poses, self.mask, True, available_actions = available_actions)
+            if 'RA' in self.args.env:
+                value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goals, edge_logits, comm_edges, probs, real_cover, ToM_target_cover=\
+                    self.model(self.state, self.hself, self.hToM, True, available_actions=available_actions)
+            else:
+                value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goals, edge_logits, comm_edges, probs, real_cover, ToM_target_cover=\
+                    self.model(self.state, self.hself, self.hToM, self.poses, self.mask, True, available_actions=available_actions)
             
             self.comm_cnt = torch.sum(comm_edges)
             self.comm_bit = self.comm_cnt * self.num_targets
@@ -198,6 +212,7 @@ class Agent(object):
         return self
 
     def reward_normalizer(self, reward):
+        print('reward', reward)
         reward = np.array(reward)
         self.num_steps += 1
         if self.num_steps == 1:
diff --git a/ToM2C/requirements.txt b/ToM2C/requirements.txt
index b85f665..0b1c1a2 100644
--- a/ToM2C/requirements.txt
+++ b/ToM2C/requirements.txt
@@ -1,6 +1,6 @@
 gym == 0.10.5
 numpy == 1.15.1
-torch == 1.7.0
+torch == 1.9.0
 tensorboard
 torchvision
 opencv-python
diff --git a/ToM2C/test.py b/ToM2C/test.py
index 90028e1..909af42 100644
--- a/ToM2C/test.py
+++ b/ToM2C/test.py
@@ -6,18 +6,20 @@ import time
 import torch
 import logging
 import numpy as np
-from tensorboardX import SummaryWriter
 
-from model import build_model
-from utils import setup_logger
-from player_util import Agent
-from environment import create_env
+import wandb
+
+from .model import build_model
+from .model_ra import build_model
+from .utils import setup_logger
+from .player_util import Agent
+from .environment import create_env
 
 
 def test(args, shared_model, optimizer, optimizer_ToM, train_modes, n_iters):
     ptitle('Test Agent')
     n_iter = 0
-    writer = SummaryWriter(os.path.join(args.log_dir, 'Test'))
+    wandb.init(project='ToM2C', name='test')
     gpu_id = args.gpu_id[-1]
     log = {}
     print(os.path.isdir(args.log_dir))
@@ -118,12 +120,11 @@ def test(args, shared_model, optimizer, optimizer_ToM, train_modes, n_iters):
                         n_iter = new_n_iter
                     # for i, r_i in enumerate(reward_sum_ep):
                     #     writer.add_scalar('test/reward'+str(i), r_i, n_iter)
-                        writer.add_scalar('test/reward', reward_sum_ep[0], n_iter)
-                        writer.add_scalar('test/fps', fps, n_iter)
+                        wandb.log({'test/reward': reward_sum_ep[0]}, step=n_iter)
+                        wandb.log({'test/fps': fps}, step=n_iter)
                     fps_all.append(fps)
                     player.clean_buffer(player.done)
                         
-                    #writer.add_scalar('test/eps_len', player.eps_len, n_iter)
                     break
         '''
             comm_cnt_list.append(comm_cnt/env.max_steps)
diff --git a/ToM2C/train.py b/ToM2C/train.py
index 3fceb27..b0b54f6 100644
--- a/ToM2C/train.py
+++ b/ToM2C/train.py
@@ -7,13 +7,14 @@ import torch.nn.functional as F
 import numpy as np
 import torch.optim as optim
 from tensorboardX import SummaryWriter
+import wandb
 from setproctitle import setproctitle as ptitle
 
 import json
-from model import build_model
-from player_util import Agent
-from environment import create_env
-from shared_optim import SharedRMSprop, SharedAdam
+from .model import build_model
+# from .player_util import Agent
+from .environment import create_env
+from .shared_optim import SharedRMSprop, SharedAdam
 
 class HLoss(nn.Module):
     def __init__(self):
@@ -30,30 +31,31 @@ class HLoss(nn.Module):
             b = -b.sum(-1)
             b = b.mean()
         return b
-
-def optimize_ToM(state, poses, masks, available_actions, args, params, optimizer_ToM, shared_model, device_share, env):
+    
+# def optimize_ToM(state, poses masks, available_actions, args, params, optimizer_ToM, shared_model, device_share, env):
+def optimize_ToM(state, masks, available_actions, args, params, optimizer_ToM, shared_model, device_share, env):
     num_agents = env.n
     num_targets = env.num_target
     max_steps = env.max_steps
     seg_num = int(max_steps/args.A2C_steps)
     if "MSMTC" in args.env:
         batch_size, num_agents, num_both, obs_dim = state.size()
-    elif "CN" in args.env:
+    elif "CN" in args.env or "RA" in args.env:
         batch_size, num_agents, obs_dim = state.size()
     count = int(batch_size/max_steps)
-    print("batch_size = ",batch_size)
-    # state, poses are only to device when being used
+    print("batch_size = ", batch_size)
+
     if "MSMTC" in args.env:
         state = state.reshape(count, max_steps, num_agents, num_both, obs_dim)#.to(device_share)
-    elif "CN" in args.env:
+    elif "CN" in args.env or "RA" in args.env:
         state = state.reshape(count, max_steps, num_agents, obs_dim)#.to(device_share)
 
-    batch_size, num_agents, num_agents, cam_dim = poses.size()
-    poses = poses.reshape(count, max_steps, num_agents, num_agents, cam_dim)#.to(device_share)
+    # batch_size, num_agents, num_agents, cam_dim = poses.size()
+    # poses = poses.reshape(count, max_steps, num_agents, num_agents, cam_dim)#.to(device_share)
  
     masks = masks.reshape(count, max_steps, num_agents, num_agents, 1)
     h_ToM = torch.zeros(count, num_agents, num_agents, args.lstm_out).to(device_share)
-    hself = torch.zeros(count, num_agents, args.lstm_out ).to(device_share)
+    hself = torch.zeros(count, num_agents, args.lstm_out).to(device_share)
     hself_start = hself.clone().detach() # save the intial hidden state for every args.num_steps
     hToM_start = h_ToM.clone().detach()
 
@@ -70,15 +72,23 @@ def optimize_ToM(state, poses, masks, available_actions, args, params, optimizer
             ToM_goals = None
             real_goals = None
             BCE_criterion = torch.nn.BCELoss(reduction='sum')
+            mse_loss = nn.MSELoss(reduction='sum')
             ToM_target_loss = torch.zeros(1).to(device_share)
             ToM_target_acc = torch.zeros(1).to(device_share)
             for s_i in range(args.A2C_steps):
                 step = seg * args.A2C_steps + s_i
                 available_action = available_actions[:,step].to(device_share) if args.mask_actions else None
                 
-                value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goal, edge_logits, comm_edges, probs, real_cover, ToM_target_cover =\
-                        shared_model(state[:,step].to(device_share), hself, h_ToM, poses[:,step].to(device_share), masks[:,step].to(device_share), available_actions = available_action)
-                ToM_target_loss += BCE_criterion(ToM_target_cover.float(), real_cover.float())
+                if "RA" in args.env:
+                    value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goal, edge_logits, comm_edges, probs, real_cover, ToM_target_cover =\
+                        shared_model(state[:, step].to(device_share), hself, h_ToM, available_actions=available_action)
+                    # print('ToM_target_cover', ToM_target_cover.shape, 'real_cover', real_cover.shape)
+                    ToM_target_loss += mse_loss(ToM_target_cover.float(), real_cover.float())
+                else:
+                    value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goal, edge_logits, comm_edges, probs, real_cover, ToM_target_cover =\
+                        shared_model(state[:, step].to(device_share), hself, h_ToM, poses[:, step].to(device_share), masks[:,step].to(device_share), available_actions = available_action)
+                    ToM_target_loss += BCE_criterion(ToM_target_cover.float(), real_cover.float())
+
                 ToM_target_cover_discrete = (ToM_target_cover > 0.6)
                 ToM_target_acc += torch.sum((ToM_target_cover_discrete == real_cover))
                 
@@ -87,16 +97,16 @@ def optimize_ToM(state, poses, masks, available_actions, args, params, optimizer
 
                 ToM_goal = ToM_goal.unsqueeze(1)
                 if "MSMTC" in args.env:
-                    real_goal = torch.cat((1-actions,actions),-1).detach()
+                    real_goal = torch.cat((1-actions, actions), -1).detach()
                     real_goal_duplicate = real_goal.reshape(count, 1, num_agents, num_targets, -1).repeat(1, num_agents, 1, 1, 1)
-                    idx= (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool()
-                    real_goal_duplicate = real_goal_duplicate[:,idx].reshape(count, 1, num_agents, num_agents-1, num_targets, -1)
-                elif "CN" in args.env:
+                    idx = (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool()
+                    real_goal_duplicate = real_goal_duplicate[:, idx].reshape(count, 1, num_agents, num_agents-1, num_targets, -1)
+                elif "CN" in args.env or "RA" in args.env:
                     real_goal = actions.reshape(count * num_agents, 1)
                     real_goal_duplicate = torch.zeros(count * num_agents, num_targets).to(device_share).scatter_(1, real_goal, 1)
                     real_goal_duplicate = real_goal_duplicate.reshape(count, 1, num_agents, num_targets, -1).repeat(1, num_agents, 1, 1, 1)
-                    idx= (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool()
-                    real_goal_duplicate = real_goal_duplicate[:,idx].reshape(count, 1, num_agents, num_agents-1, num_targets)
+                    idx = (torch.ones(num_agents, num_agents) - torch.diag(torch.ones(num_agents))).bool()
+                    real_goal_duplicate = real_goal_duplicate[:, idx].reshape(count, 1, num_agents, num_agents-1, num_targets)
                 if ToM_goals is None:
                     ToM_goals = ToM_goal
                     real_goals = real_goal_duplicate
@@ -107,9 +117,11 @@ def optimize_ToM(state, poses, masks, available_actions, args, params, optimizer
             KL_criterion = torch.nn.KLDivLoss(reduction='sum')
             real_prob = real_goals.float()
             ToM_prob = ToM_goals.float()
+            # print('real_prob', real_prob)
+            # print('ToM_prob', ToM_prob.shape, 'real_prob', real_prob.shape)
             ToM_loss += KL_criterion(ToM_prob.log(), real_prob)
             
-            loss = ToM_loss + 0.5 * ToM_target_loss
+            loss = ToM_loss # + 0.5 * ToM_target_loss
             loss = loss/(count)
             shared_model.zero_grad()
             loss.backward()
@@ -131,7 +143,7 @@ def optimize_ToM(state, poses, masks, available_actions, args, params, optimizer
     ToM_target_acc_mean = ToM_target_acc_sum/cnt_all
     return ToM_loss_sum, ToM_loss_mean, ToM_target_loss_mean, ToM_target_acc_mean  
 
-def optimize_Policy(state, poses, real_actions, reward, masks, available_actions, args, params, optimizer_Policy, shared_model, device_share, env):
+def optimize_Policy(state, real_actions, reward, masks, available_actions, args, params, optimizer_Policy, shared_model, device_share, env):
     num_agents = env.n
     num_targets = env.num_target
     max_steps = env.max_steps
@@ -139,7 +151,7 @@ def optimize_Policy(state, poses, real_actions, reward, masks, available_actions
     seg_num = int(max_steps/args.A2C_steps)
     if "MSMTC" in args.env:
         batch_size, num_agents, num_both, obs_dim = state.size()
-    elif "CN" in args.env:
+    elif "CN" in args.env or "RA" in args.env:
         batch_size, num_agents, obs_dim = state.size()
     count = int(batch_size/max_steps)
 
@@ -151,12 +163,12 @@ def optimize_Policy(state, poses, real_actions, reward, masks, available_actions
     if "MSMTC" in args.env:
         state = state.reshape(count, max_steps, num_agents, num_both, obs_dim)#.to(device_share)
         real_actions = real_actions.reshape(count, max_steps, num_agents, num_targets, 1)#.to(device_share)
-    elif "CN" in args.env:
+    elif "CN" in args.env or "RA" in args.env:
         state = state.reshape(count, max_steps, num_agents, obs_dim)#.to(device_share)
         real_actions = real_actions.reshape(count, max_steps, num_agents, 1)#.to(device_share)
 
-    batch_size, num_agents, num_agents, cam_dim = poses.size()
-    poses = poses.reshape(count, max_steps, num_agents, num_agents, cam_dim)#.to(device_share)
+    # batch_size, num_agents, num_agents, cam_dim = poses.size()
+    # poses = poses.reshape(count, max_steps, num_agents, num_agents, cam_dim)#.to(device_share)
     batch_size, num_agents, r_dim = reward.size()
     reward = reward.reshape(count, max_steps, num_agents, r_dim)#.to(device_share)
 
@@ -164,7 +176,6 @@ def optimize_Policy(state, poses, real_actions, reward, masks, available_actions
     h_ToM = torch.zeros(count, num_agents, num_agents, args.lstm_out).to(device_share)
     
     hself = torch.zeros(count, num_agents, args.lstm_out ).to(device_share)
-    #hothers = torch.zeros(count, num_agents, num_agents-1, args.lstm_out).to(device_share)
     hself_start = hself.clone().detach()  # save the intial hidden state for every args.num_steps
     hToM_start = h_ToM.clone().detach()
     if args.mask_actions:
@@ -190,7 +201,9 @@ def optimize_Policy(state, poses, real_actions, reward, masks, available_actions
                 
                 if "ToM2C" in args.model:
                     value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goal, edge_logit, comm_edges, probs, real_cover, ToM_target_cover =\
-                            shared_model(state[:,step].to(device_share), hself, h_ToM, poses[:,step].to(device_share), masks[:,step].to(device_share), available_actions= available_action)
+                            shared_model(state[:, step].to(device_share), hself, h_ToM, available_actions=available_action)
+                    # value_multi, actions, entropy, log_prob, hn_self, hn_ToM, ToM_goal, edge_logit, comm_edges, probs, real_cover, ToM_target_cover =\
+                    #         shared_model(state[:,step].to(device_share), hself, h_ToM, poses[:,step].to(device_share), masks[:,step].to(device_share), available_actions= available_action)
                     hself = hn_self
                     hToM = hn_ToM        
                 
@@ -198,7 +211,6 @@ def optimize_Policy(state, poses, real_actions, reward, masks, available_actions
                 entropies.append(entropy)
                 log_probs.append(torch.log(probs).gather(-1, real_actions[:,step].to(device_share)))
                 rewards.append(reward[:,step].to(device_share))
-
                 edge_logits.append(edge_logit)
 
             R = torch.zeros(count, num_agents, 1).to(device_share)
@@ -206,52 +218,54 @@ def optimize_Policy(state, poses, real_actions, reward, masks, available_actions
                 # not the last segment of the episode
                 next_step = (seg+1) * args.A2C_steps
                 available_action = available_actions[:,next_step].to(device_share) if args.mask_actions else None
-                value_multi, *others = shared_model(state[:,next_step].to(device_share), hself, h_ToM, poses[:,next_step].to(device_share), masks[:,next_step].to(device_share), available_actions= available_action)
+                # value_multi, *others = shared_model(state[:,next_step].to(device_share), hself, h_ToM, poses[:,next_step].to(device_share), masks[:,next_step].to(device_share), available_actions= available_action)
+                value_multi, *others = shared_model(state[:,next_step].to(device_share), hself, h_ToM, available_actions= available_action)
                 R = value_multi.clone().detach()
 
             R = R.to(device_share)
             values.append(R)
-
             policy_loss = torch.zeros(count, num_agents, num_targets, 1).to(device_share)
             value_loss = torch.zeros(count, num_agents, 1).to(device_share)
             entropies_sum = torch.zeros(1).to(device_share)
             w_entropies = float(args.entropy)
-
             Sparsity_loss = torch.zeros(count, 1).to(device_share)
 
             criterionH = HLoss()
             edge_prior = torch.FloatTensor(np.array([0.7, 0.3])).to(device_share)
             gae = torch.zeros(count, num_agents, 1).to(device_share)
 
-            for i in reversed(range(args.A2C_steps)):
+            for i in reversed(range(2, args.A2C_steps)):
                 R = args.gamma * R + rewards[i]
                 advantage = R - values[i]
                 value_loss = value_loss + 0.5 * advantage.pow(2)
                 # Generalized Advantage Estimataion
                 delta_t = rewards[i] + args.gamma * values[i + 1].data - values[i].data
+                print(i, 'rew', rewards[i].mean(), 'delta_t', delta_t.mean())
                 gae = gae * args.gamma * args.tau + delta_t
-                #value_loss = value_loss + 0.5 * (gae + values[i].data -values[i]).pow(2)
 
                 if "MSMTC" in args.env:
                     gae_duplicate = gae.unsqueeze(2).repeat(1,1,num_targets,1)
                     policy_loss = policy_loss - (w_entropies * entropies[i]) - (log_probs[i] * gae_duplicate)
-                elif "CN" in args.env:
+                elif "CN" in args.env or "RA" in args.env:
                     gae_duplicate = gae
-                    if policy_loss.sum() == 0 : policy_loss = torch.zeros(1).to(device_share)
+                    if policy_loss.sum() == 0: policy_loss = torch.zeros(1).to(device_share)
+                    print(i, 'gae_duplicate', gae_duplicate.min(), (log_probs[i] * gae_duplicate).sum())
                     policy_loss = policy_loss - (w_entropies * entropies[i].sum()) - (log_probs[i] * gae_duplicate).sum()
 
                 entropies_sum += entropies[i].sum()
 
-                edge_logit = edge_logits[i]#.reshape(count * num_agents * num_agents, -1)  # k * 2
+                edge_logit = edge_logits[i] #.reshape(count * num_agents * num_agents, -1)  # k * 2
                 Sparsity_loss += -criterionH(edge_logit, edge_prior)
             
             shared_model.zero_grad()
-            loss = policy_loss.sum() + 0.5 * value_loss.sum() #+ 0.3 * Sparsity_loss.sum()
+
+            loss = policy_loss.sum() + 1e-4 * value_loss.sum() #+ 0.3 * Sparsity_loss.sum()
             loss = loss/(count * 4)
             loss.backward()
 
             torch.nn.utils.clip_grad_norm_(params, 5)
             optimizer_Policy.step()
+
         # update hself & hothers start for next segment
         hself_start = hself.clone().detach()
         hToM_start = h_ToM.clone().detach()
@@ -261,7 +275,7 @@ def optimize_Policy(state, poses, real_actions, reward, masks, available_actions
         Sparsity_loss_sum += Sparsity_loss
         entropies_all += entropies_sum
 
-    return policy_loss_sum, value_loss_sum, Sparsity_loss_sum, entropies_all
+    return policy_loss_sum, value_loss_sum, Sparsity_loss_sum, entropies_all, actions
 
 def reduce_comm(policy_data, args, params_comm, optimizer, lr_scheduler, shared_model, ori_model, device_share, env):
     state, poses, real_actions, reward, comm_domains, available_actions = policy_data
@@ -320,8 +334,6 @@ def reduce_comm(policy_data, args, params_comm, optimizer, lr_scheduler, shared_
             hself = hn_self
             hToM = hn_ToM        
             
-            #print(curr_edges)
-            #print(best_edges)
             # idx = (best_edges == 1)
             # if curr_edges[idx].size()[0] > 0:
             #     print(torch.sum(1-curr_edges[idx])/curr_edges[idx].size()[0])
@@ -351,7 +363,7 @@ def reduce_comm(policy_data, args, params_comm, optimizer, lr_scheduler, shared_
             loss_1 = CE_criterion(logit_1, label_1.long()) if size_1 > 0 else 0
             #print(CE_criterion(edge_logit,edge_label.long()), loss_0+loss_1)
             comm_loss +=  loss_0 + loss_1
-        #print(logit_0[:5,0].reshape(-1).data)
+
         shared_model.zero_grad()
         comm_loss.backward()#retain_graph=True)
         torch.nn.utils.clip_grad_norm_(params_comm, 20)
@@ -371,7 +383,7 @@ def load_data(args, history):
     data_list = [[] for i in range(item_cnt)]
 
     for history in history_list:
-        for i,item in enumerate(history):
+        for i, item in enumerate(history):
             data_list[i].append(history[item])
 
     for i in range(item_cnt):
@@ -383,7 +395,8 @@ def load_data(args, history):
 
 def train(args, shared_model, optimizer_Policy, optimizer_ToM, train_modes, n_iters, curr_env_steps, ToM_count, ToM_history, Policy_history, step_history, loss_history, env=None):
     rank = args.workers
-    writer = SummaryWriter(os.path.join(args.log_dir, 'Train'))
+
+    wandb.init(project='ToM2C', name='train')
     ptitle('Training')
     gpu_id = args.gpu_id[rank % len(args.gpu_id)]
     torch.manual_seed(args.seed + rank)
@@ -399,14 +412,14 @@ def train(args, shared_model, optimizer_Policy, optimizer_ToM, train_modes, n_it
             device_share = torch.device('cuda:' + str(args.gpu_id[-1]))
     else:
         device_share = torch.device('cpu')
-    #device_share = torch.device('cuda:0')
+
     if env == None:
         env = create_env(env_name, args)
 
     params = []
     params_ToM = []
     params_comm = []
-    for name,param in shared_model.named_parameters():
+    for name, param in shared_model.named_parameters():
         if 'ToM' in name or 'other' in name:
             params_ToM.append(param)
         else:
@@ -450,24 +463,30 @@ def train(args, shared_model, optimizer_Policy, optimizer_ToM, train_modes, n_it
         if args.train_comm:
             data_list = load_data(args, Policy_history)
             comm_loss = reduce_comm(data_list, args, params_comm, optimizer_comm, lr_scheduler, shared_model, ori_model, device_share, env)
-            writer.add_scalar('train/comm_loss', comm_loss.sum(), sum(n_iters))
+            wandb.log({'train/comm_loss': comm_loss.sum()}, step=sum(n_iters))
             print("comm_loss:", comm_loss.item())
             if comm_loss.sum() < 1:
                 break
         else:
             train_step_cnt += 1
-            state, poses, real_actions, reward, masks, available_actions = load_data(args, Policy_history)
+            # state, poses, real_actions, reward, masks, available_actions = load_data(args, Policy_history)
+            state, real_actions, reward, masks, available_actions = load_data(args, Policy_history)
 
-            policy_loss, value_loss, Sparsity_loss, entropies_sum =\
-                optimize_Policy(state, poses, real_actions, reward, masks, available_actions, args, params, optimizer_Policy, shared_model, device_share, env)
+            # policy_loss, value_loss, Sparsity_loss, entropies_sum =\
+            #     optimize_Policy(state, poses, real_actions, reward, masks, available_actions, args, params, optimizer_Policy, shared_model, device_share, env)
+            policy_loss, value_loss, Sparsity_loss, entropies_sum, actions =\
+                optimize_Policy(state, real_actions, reward, masks, available_actions, args, params, optimizer_Policy, shared_model, device_share, env)
+            
+            action_correct_rate = torch.eq(actions, torch.tensor([[0], [1], [1]]))
 
             # log training information
             n_steps = sum(n_iters)  # global_steps_count
-            writer.add_scalar('train/policy_loss_sum', policy_loss.sum(), n_steps)
-            writer.add_scalar('train/value_loss_sum', value_loss.sum(), n_steps)
-            writer.add_scalar('train/Sparsity_loss_sum', Sparsity_loss.sum(), n_steps)
-            writer.add_scalar('train/entropies_sum', entropies_sum.sum(), n_steps)
-            writer.add_scalar('train/gamma', args.gamma, n_steps)
+            wandb.log({'train/policy_loss_sum': policy_loss.sum()}, step=n_steps)
+            wandb.log({'train/value_loss_sum': value_loss.sum()}, step=n_steps)
+            wandb.log({'train/Sparsity_loss_sum': Sparsity_loss.sum()}, step=n_steps)
+            wandb.log({'train/entropies_sum': entropies_sum.sum()}, step=n_steps)
+            wandb.log({'train/gamma': args.gamma}, step=n_steps)
+            wandb.log({'train/action_correct_rate': action_correct_rate.float().mean()}, step=n_steps)
             print("policy loss:{}".format(policy_loss.sum().data))
             print("value loss:{}".format(value_loss.sum().data))
             print("entropies:{}".format(entropies_sum.sum().data))
@@ -478,15 +497,15 @@ def train(args, shared_model, optimizer_Policy, optimizer_ToM, train_modes, n_it
             if 'ToM2C' in args.model:
                 if sum(ToM_count) >= ToM_len:
                     print("ToM training started")
-                    state, poses, masks, real_goals, available_actions = load_data(args, ToM_history)
+                    state, masks, real_goals, available_actions = load_data(args, ToM_history)
                     print("ToM data loaded")
-                    ToM_loss_sum, ToM_loss_avg, ToM_target_loss, ToM_target_acc = optimize_ToM(state, poses, masks, available_actions, args, params_ToM, optimizer_ToM, shared_model, device_share, env)
+                    ToM_loss_sum, ToM_loss_avg, ToM_target_loss, ToM_target_acc = optimize_ToM(state, masks, available_actions, args, params_ToM, optimizer_ToM, shared_model, device_share, env)
                     print("optimized based on ToM loss")
-                    
-                    writer.add_scalar('train/ToM_loss_sum', ToM_loss_sum.sum(), n_steps)
-                    writer.add_scalar('train/ToM_loss_avg', ToM_loss_avg.sum(), n_steps)
-                    writer.add_scalar('train/ToM_target_loss_avg', ToM_target_loss.sum(), n_steps)
-                    writer.add_scalar('train/ToM_target_acc_avg', ToM_target_acc.sum(), n_steps)
+
+                    wandb.log({'train/ToM_loss_sum': ToM_loss_sum.sum()}, step=n_steps)
+                    wandb.log({'train/ToM_loss_avg': ToM_loss_avg.sum()}, step=n_steps)
+                    wandb.log({'train/ToM_target_loss_avg': ToM_target_loss.sum()}, step=n_steps)
+                    wandb.log({'train/ToM_target_acc_avg': ToM_target_acc.sum()}, step=n_steps)
 
                     for rank in range(args.workers):
                         ToM_history[rank] = []
diff --git a/ToM2C/worker.py b/ToM2C/worker.py
index 7cdf628..d9fede2 100644
--- a/ToM2C/worker.py
+++ b/ToM2C/worker.py
@@ -7,9 +7,9 @@ import torch.optim as optim
 from tensorboardX import SummaryWriter
 from setproctitle import setproctitle as ptitle
 
-from model import build_model
-from player_util import Agent
-from environment import create_env
+from .model import build_model
+from .player_util import Agent
+from .environment import create_env
 
 
 def worker(rank, args, shared_model, train_modes, n_iters, curr_env_steps, ToM_count, ToM_history, Policy_history, step_history, loss_history, env=None):
@@ -56,10 +56,10 @@ def worker(rank, args, shared_model, train_modes, n_iters, curr_env_steps, ToM_c
     ave_reward = np.zeros(2)
     ave_reward_longterm = np.zeros(2)
     count_eps = 0
-    #max_steps = env.max_steps
+    # max_steps = env.max_steps
     while True:
         if "MSMTC" in args.env and args.random_target:
-            p = 0.7 - (env.max_steps/20 -1) * 0.1
+            p = 0.7 - (env.max_steps / 20 - 1) * 0.1
         
             env.target_type_prob = [p, 1-p]
             player.env.target_type_prob = [p, 1-p]
diff --git a/crafter/config.py b/crafter/config.py
index 35c0914..4b5b99e 100644
--- a/crafter/config.py
+++ b/crafter/config.py
@@ -39,7 +39,7 @@ game_arg.add_argument('--TUp', type=int, default=100, help='duration of one GAME
 game_arg.add_argument('--demandDistribution', type=int, default=0, help='0=uniform, 1=normal distribution, 2=the sequence of 4,4,4,4,8,..., 3= basket data, 4= forecast data')
 game_arg.add_argument('--scaled', type=str2bool, default=False, help='if true it uses the (if) existing scaled parameters')
 game_arg.add_argument('--demandLow', type=int, default=[4, 0, 0], help='the lower bound of random demand')
-game_arg.add_argument('--demandUp', type=int, default=[16, 6, 6], help='the upper bound of random demand')
+game_arg.add_argument('--demandUp', type=int, default=[16, 8, 6], help='the upper bound of random demand')
 game_arg.add_argument('--demandMu', type=float, default=10, help='the mu of the normal distribution for demand ')
 game_arg.add_argument('--demandSigma', type=float, default=2, help='the sigma of the normal distribution for demand ')
 game_arg.add_argument('--actionMax', type=int, default=2, help='it works when fixedAction is True')
diff --git a/crafter/data.yaml b/crafter/data.yaml
index 5eb57a8..61edfd1 100644
--- a/crafter/data.yaml
+++ b/crafter/data.yaml
@@ -39,12 +39,12 @@ walkable:
   - sand
 
 items:
-  health: {max: 15, initial: 9}
-  food: {max: 50, initial: 9}
-  drink: {max: 50, initial: 9}
+  health: {max: 25, initial: 9}
+  food: {max: 60, initial: 9}
+  drink: {max: 60, initial: 9}
   energy: {max: 9, initial: 9}
   player: {max: 9, initial: 9}
-  staff: {max: 9, initial: 9}
+  staff: {max: 25, initial: 9}
   death: {max: 9, initial: 0}
   sapling: {max: 9, initial: 0}
   wood: {max: 9, initial: 0}
diff --git a/crafter/engine.py b/crafter/engine.py
index 885b124..1059b10 100644
--- a/crafter/engine.py
+++ b/crafter/engine.py
@@ -91,6 +91,8 @@ class World:
         # Upstream
         self.station.OO[resource] = sum(self.station.AS[resource])
 
+    print(self.shelter.OO, self.station.OO, self.warehouse.OO)
+
   def __setitem__(self, pos, material):
     if material not in self._mat_ids:
       id_ = len(self._mat_ids)
diff --git a/crafter/env.py b/crafter/env.py
index 282bc2d..9baa220 100644
--- a/crafter/env.py
+++ b/crafter/env.py
@@ -125,11 +125,10 @@ class Env(BaseClass):
     return constants.actions
 
   def reset(self):
-    self.reset_callback(self.world)
-    # center = (self.world.area[0] // 2, self.world.area[1] // 2)
     self._episode += 1
     self._step = 0
     self.world.reset(seed=hash((self._seed, self._episode)) % (2 ** 31 - 1))
+    self.reset_callback(self.world)
     self._update_time()
 
     self._unlocked = set()
@@ -138,6 +137,8 @@ class Env(BaseClass):
     obs_n = []
     for agent in self.players:
       obs_n.append(self._get_obs(agent))
+
+    self.world.update_OO()
     return obs_n
 
   # get reward for a particular agent
@@ -176,12 +177,11 @@ class Env(BaseClass):
     for agent in self.players:
       agent.step(self._step)
       self.update_agent_state(agent)
-    
-    self.world.update_OO()
 
     communications = []
     for requester in self.players:
       if requester.out_requests:
+        print('out_requests', requester.out_requests)
         # TODO: make this more efficient and less hard-coding
         for request in requester.out_requests:
           communications.extend(requester.out_requests)
@@ -225,7 +225,7 @@ class Env(BaseClass):
     #   reward_n = [reward] * self.n
 
     # done = self._length and (self._step >= self._length)
-    done = False
+    done = self._step >= 20
     info = {
         'inventory': self.world._player.inventory.copy(),
         'semantic': self._sem_view(),
diff --git a/crafter/objects.py b/crafter/objects.py
index a5476a5..706ab07 100644
--- a/crafter/objects.py
+++ b/crafter/objects.py
@@ -102,26 +102,26 @@ class Agency:
     elif self.config.demandDistribution == 1 or self.config.demandDistribution == 3 or self.config.demandDistribution == 4:
       self.a_b = self.config.demandMu # parameters for the formula
       self.b_b = self.config.demandMu*(np.mean((self.config.leadRecItemLow[self.agentNum], 
-			self.config.leadRecItemUp[self.agentNum])) + np.mean((self.config.leadRecOrderLow[self.agentNum], self.config.leadRecOrderUp[self.agentNum]))) # parameters for the formula
+			  self.config.leadRecItemUp[self.agentNum])) + np.mean((self.config.leadRecOrderLow[self.agentNum], self.config.leadRecOrderUp[self.agentNum]))) # parameters for the formula
     elif self.config.demandDistribution == 2:
       self.a_b = 8 # parameters for the formula
       self.b_b = (3/4.)*8*(np.mean((self.config.leadRecItemLow[self.agentNum] , 
-			self.config.leadRecItemUp[self.agentNum])) + np.mean((self.config.leadRecOrderLow[self.agentNum] , self.config.leadRecOrderUp[self.agentNum]))) # parameters for the formula
+			  self.config.leadRecItemUp[self.agentNum])) + np.mean((self.config.leadRecOrderLow[self.agentNum] , self.config.leadRecOrderUp[self.agentNum]))) # parameters for the formula
     elif self.config.demandDistribution == 3:
       self.a_b = 10 # parameters for the formula
-      self.b_b = 7*(np.mean((self.config.leadRecItemLow[self.agentNum] , 
-			self.config.leadRecItemUp[self.agentNum])) + np.mean((self.config.leadRecOrderLow[self.agentNum] , self.config.leadRecOrderUp[self.agentNum]))) # parameters for the formula
+      self.b_b = 7 * (np.mean((self.config.leadRecItemLow[self.agentNum], 
+			  self.config.leadRecItemUp[self.agentNum])) + np.mean((self.config.leadRecOrderLow[self.agentNum] , self.config.leadRecOrderUp[self.agentNum]))) # parameters for the formula
 
   @property
   def texture(self):
     raise 'unknown'
   
   # updates the IL and OO at time t, after recieving "rec" number of items 
-  def receiveItems(self, time):
+  def receiveItems(self):
     for resource in self.base_stock.keys():
-      self.inventory[resource] = int(self.inventory[resource] + self.AS[resource][time]) # inverntory level update
-      self.OO[resource] = min(0, int(self.OO[resource] - self.AS[resource][time])) # invertory in transient update
-
+      self.inventory[resource] = int(self.inventory[resource] + self.AS[resource][self.curTime]) # inverntory level update
+      self.OO[resource] = max(0, int(self.OO[resource] - self.AS[resource][self.curTime])) # invertory in transient update
+    
   def _process_requests(self):
     requests = []
     for request in self.in_requests:
@@ -139,7 +139,7 @@ class Agency:
     return requests
   
   def _make_orders(self, goal):
-    print(self, goal)
+    # print(self, goal)
     self._communication = 0
     order = {}
     if self.strategy == 'bs':
@@ -181,12 +181,14 @@ class Agency:
     if 'staff' in order.keys():
       if self.name is not 'Station':
         self.out_requests.append(f"{self.name}->Station: Please send {order['staff']} staff")
+        self.OO['staff'] += order['staff']
         self._communication += 1
       del order['staff']
 
     request = f'{self.name}->Warehouse: Please send '
     for resource, quantity in order.items():
       if resource is not 'staff':
+        self.OO[resource] += quantity
         request += str(quantity) + ' ' + resource + ' '
 
     if order.keys():
@@ -195,6 +197,7 @@ class Agency:
     
     self.curTime += 1
     self.cumReward = self.gamma * self.cumReward + self.curReward
+    
 
   def resetPlayer(self, T):
     self.OO = {key: 0 for key in self.base_stock.keys()}
@@ -215,6 +218,9 @@ class Agency:
     # if self.compTypeTrain == 'srdqn':
     #   self.brain.setInitState(self.curObservation) # sets the initial input of the network
     self.curTime = 0
+    self.staff_team = []
+    self.patients = []
+
 
   # This function returns a np.array of the current state of the agent	
   def getCurState(self, t=None):
@@ -701,7 +707,6 @@ class Person:
       self.inventory[name] = max(0, min(amount, maxmium))
 
   def _update_life_stats(self):
-    import pdb; pdb.set_trace()
     self._hunger += 0.5 if self.sleeping else 1
     if self._hunger > 25:
       self._hunger = 0
@@ -832,7 +837,6 @@ class Warehouse(Agency):
     self.world = world
     self.pos = np.array(pos)
     self.random = world.random
-    # self.inventory = {name: info['initial'] for name, info in constants.items.items()}
 
     self.inventory = {'food': time_varying_demand_supply.demand(mean=40, std_dev=2), 
                       'drink': time_varying_demand_supply.demand(mean=40, std_dev=2), 
@@ -861,9 +865,20 @@ class Warehouse(Agency):
   @property
   def reward(self):
     return self.curReward
-
+  
+  def resetPlayer(self, T):
+    super().resetPlayer(T)
+    self.inventory = {'food': time_varying_demand_supply.demand(mean=40, std_dev=2), 
+                      'drink': time_varying_demand_supply.demand(mean=40, std_dev=2), 
+                      'staff': 9, 
+                      'wood': 0, 
+                      'stone': 0, 
+                      'coal': 0}
+    self.staff_team = [Person('staff', 5) for _ in range(self.inventory['staff'])]
+    return 
+  
   def step(self, _step):
-    self.receiveItems(_step)
+    self.receiveItems()
     self._update_life_stats()
     
     self.curReward = - self._backorder - self._communication
@@ -911,14 +926,14 @@ class Warehouse(Agency):
         self.AO[resource][self.curTime] = total_quantity
         if self.inventory[resource] - self.base_stock[resource] >= total_quantity: 
           for requester, quantity in requests_list:
-            requester.inventory[resource] += quantity
-            requester.AS[resource][self.curTime] += quantity
+            # requester.inventory[resource] += quantity
+            requester.AS[resource][self.curTime + 1] += quantity
         else:
           average_quantity = total_quantity // len(requests_list)
           self.inventory[resource] -= total_quantity
           for requester, _ in requests_list:
             requester.inventory[resource] += average_quantity
-            requester.AS[resource][self.curTime] += average_quantity
+            requester.AS[resource][self.curTime + 1] += average_quantity
 
     self.in_requests = [] # Clear
 
@@ -930,20 +945,17 @@ class Shelter(Agency):
   def __init__(self, world, pos, agentNum, config):
     super().__init__(world, pos, agentNum, config)
     # self.inventory = {name: info['initial'] for name, info in constants.items.items()}
-    number_initial_patients = time_varying_demand_supply.demand(mean = 10, std_dev = 2)
-    number_initial_staff = time_varying_demand_supply.demand(mean=10, std_dev=2)
-
-    self.patients = [Person('injured', 0) for _ in range(number_initial_patients)]
-    self.staff_team = [Person('staff', 5) for _ in range(number_initial_staff)]
-    self.inventory = {'health': number_initial_patients, 
+    
+    self.inventory = {'health': time_varying_demand_supply.demand(mean = 10, std_dev = 2), 
                       'food': 39, 
                       'drink': 39, 
-                      'staff': number_initial_staff, 
+                      'staff': time_varying_demand_supply.demand(mean=10, std_dev=2), 
                       'death': 0,
                       'wood': 0, 
                       'stone': 0, 
                       'coal': 0}
-    
+    self.patients = [Person('injured', 0) for _ in range(self.inventory['health'])]
+    self.staff_team = [Person('staff', 5) for _ in range(self.inventory['staff'])]
     self.base_stock = {'food': 30, 
                       'drink': 30, 
                       'staff': 12, 
@@ -968,10 +980,27 @@ class Shelter(Agency):
   def reward(self):
     return self.curReward
   
+  def resetPlayer(self, T):
+    super().resetPlayer(T)
+    self.inventory = {'health': time_varying_demand_supply.demand(mean=10, std_dev=2), 
+                      'food': 39, 
+                      'drink': 39, 
+                      'staff': time_varying_demand_supply.demand(mean=10, std_dev=2), 
+                      'death': 0,
+                      'wood': 0, 
+                      'stone': 0, 
+                      'coal': 0}
+    self.patients = [Person('injured', 0) for _ in range(self.inventory['health'])]
+    self.staff_team = [Person('staff', 5) for _ in range(self.inventory['staff'])]
+    self.base_stock = {'food': 30, 
+                      'drink': 30, 
+                      'staff': 12, 
+                      } 
+  
   def step(self, _step):
     self._death = 0
     self._helped_people = 0
-    self.receiveItems(_step)
+    self.receiveItems()
 
     new_arrived_injure = time_varying_demand_supply.piecewise_function(_step)
     self.inventory['health'] += new_arrived_injure
@@ -985,10 +1014,11 @@ class Shelter(Agency):
 
     for name, amount in self.inventory.items():
       maxmium = constants.items[name]['max']
-      self.inventory[name] = max(0, min(amount, maxmium))
+      # self.inventory[name] = max(0, min(amount, maxmium))
+      self.inventory[name] = min(amount, maxmium)
 
-    # print('Day:', _step, [patient.health for patient in self.patients], self.inventory['health'], len(self.staff_team), self.inventory['food'], self.inventory['drink'])
-    # print([patient._admitted_days for patient in self.patients])
+    print('Day:', _step, [patient.health for patient in self.patients], self.inventory['health'], len(self.staff_team), self.inventory['food'], self.inventory['drink'])
+    print([patient._admitted_days for patient in self.patients])
 
   def _update_patient_inventory_stats(self):
     self.consumption = 0
@@ -1001,9 +1031,11 @@ class Shelter(Agency):
       patient, staff = self.patients[i], self.staff_team[i]
       if patient.health < 5:
         staff.health -= 1
+        self.AO['staff'][self.curTime] += 1
         # Consume food
         if self.inventory['food'] > 0:
           self.inventory['food'] -= 1
+          self.AO['food'][self.curTime] += 1
           patient.health += 0.5
           self.consumption += 1
           self._helped_people += 1
@@ -1011,6 +1043,7 @@ class Shelter(Agency):
         # Consume water
         if self.inventory['drink'] > 0:
           self.inventory['drink'] -= 1
+          self.AO['drink'][self.curTime] += 1
           patient.health += 0.5
           self.consumption += 1
           self._helped_people += 1
@@ -1023,14 +1056,13 @@ class Shelter(Agency):
         self._death += 1
         self.inventory['health'] -= 1
 
-    self.patients = [patient for patient in self.patients if patient._admitted_days < 5 or patient.health >= 2]
+    self.patients = [patient for patient in self.patients if patient._admitted_days < 5] # or patient.health >= 2
 
   def _update_staff_stats(self):
     for _ in range(max(0, self.inventory['staff'] - len(self.staff_team))):
       self.staff_team.append(Person('staff', 5))
     
     returning_staff = 0
-    food_request, drink_request = 0, 0
     for staff in self.staff_team:
       if staff.health <= 0:
         self.staff_team.remove(staff)
@@ -1080,15 +1112,14 @@ class Station(Agency):
     self.world = world
     self.pos = np.array(pos)
     self.random = world.random
-
-    number_staff = time_varying_demand_supply.demand(mean=12, std_dev=2)
-    self.staff_team = [Person('staff', 5) for _ in range(number_staff)]
+    
     self.inventory = {'food': 9, 
                       'drink': 9, 
-                      'staff': number_staff, 
+                      'staff': time_varying_demand_supply.demand(mean=12, std_dev=2), 
                       'wood': 0, 
                       'stone': 0, 
                       'coal': 0}
+    self.staff_team = [Person('staff', 5) for _ in range(self.inventory['staff'])]
 
     self.achievements = {name: 0 for name in constants.achievements}
     self.action = 'noop'
@@ -1110,17 +1141,29 @@ class Station(Agency):
   @property
   def reward(self):
     return self.curReward
+  
+  def resetPlayer(self, T):
+    super().resetPlayer(T)
+    
+    self.inventory = {'food': 9, 
+                      'drink': 9, 
+                      'staff': time_varying_demand_supply.demand(mean=12, std_dev=2), 
+                      'wood': 0, 
+                      'stone': 0, 
+                      'coal': 0}
+    self.staff_team = [Person('staff', 5) for _ in range(self.inventory['staff'])]
+    return 
 
   def step(self, _step):
     self._backorder = 0
-    self.receiveItems(_step)
+    self.receiveItems()
     self._update_inventory_stats()
     self.curReward = - self._backorder - self._communication
 
-    # for name, amount in self.inventory.items():
-    #   maxmium = constants.items[name]['max']
-    #   self.inventory[name] = max(0, min(amount, maxmium))
-
+    for name, amount in self.inventory.items():
+      maxmium = constants.items[name]['max']
+      # self.inventory[name] = max(0, min(amount, maxmium))
+      self.inventory[name] = min(amount, maxmium)
   def _update_inventory_stats(self):
     self.consumption = 0
     for _ in range(max(self.inventory['staff'] - len(self.staff_team), 0)):
@@ -1148,7 +1191,7 @@ class Station(Agency):
           while self.inventory[resource] > self.base_stock[resource] and self.staff_team and self.staff_team[0].health > 4:
             self.inventory[resource] -= 1
             requester.inventory[resource] += 1
-            requester.AS[resource][self.curTime] += 1
+            requester.AS[resource][self.curTime + 1] += 1
             self.staff_team.pop(0)
     
     self.in_requests = []
diff --git a/main.py b/main.py
index 8f7f3f4..438a08d 100644
--- a/main.py
+++ b/main.py
@@ -178,5 +178,5 @@ def start():
 
 
 if __name__=='__main__':
-    #os.environ["WANDB_MODE"] = "disabled"
+    # os.environ["WANDB_MODE"] = "disabled"
     start()
\ No newline at end of file
diff --git a/multiagent/scenarios/RA.py b/multiagent/scenarios/RA.py
index bd5de44..3d1e2e9 100644
--- a/multiagent/scenarios/RA.py
+++ b/multiagent/scenarios/RA.py
@@ -45,7 +45,10 @@ class Scenario(BaseScenario):
             for key, item in agent.base_stock.items():
                 landmark.extend([0, item, 0])
             world.landmarks.append(landmark)
-
+        
+        world.landmarks = [[0, 50, 0, 0, 50, 0, 0, 20, 0],
+                           [0, 30, 0, 0, 30, 0, 0, 12, 0], 
+                           [0, 2, 0, 0, 2, 0, 0, 1, 0]]
         # make initial conditions
         world = self.reset_world(world)
         
@@ -99,13 +102,13 @@ class Scenario(BaseScenario):
         global reward
         """
         rew = 0
-        self.comm_weight, self.cons_weight = 1, 1
+        self.comm_weight, self.cons_weight = 0.5, 0.5
         for agent in world.agents:
             rew += - self.comm_weight * agent._communication
             rew += - self.cons_weight * agent.consumption
-        rew += -1 * world.shelter._death
-        # rew += 1 * world.shelter._helped_people
-        
+        rew += -10 * world.shelter._death
+        # rew += 2 * world.shelter._helped_people
+        if world.shelter.inventory['health'] == 0: rew = 0
         return rew
     
     def reward(self, agent, world):
@@ -122,7 +125,7 @@ class Scenario(BaseScenario):
 
         for entity in world.landmarks:
             entity_pos.append(entity - agent.getCurState())
-        #print(agent.getCurState())
+        print('obs', agent, agent.getCurState())
         return np.concatenate([agent.getCurState()] + entity_pos)
     
     def rule_policy(self, obs):
